task_id,type,domain_knowledge,dataset,Arcpy,prompt_content
1,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find heat islands and at-risk populations in Madison, Wisconsin 
[Instruction]: 
Your task is analyzing urban heat using Kriging interpolation techniques in Python. The analysis 
should focus on understanding spatial patterns of urban heat islands by using point temperature 
data and interpolating these values across a city. You will have to use a demographic layer to extract 
and enhance the data visualization on the elder group(Age>65). The goal is to load the temperature 
sample data, apply the Kriging method to predict temperature across the urban area, and generate 
a choropleth map showing the average interpolated temperature surface in each census block group. 
Highlighting the area with high interpolated area as well as high density of the elder population. 
The final output should be saved as ""pred_results/interpolated_urban_heat.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
1,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find heat islands and at-risk populations in Madison, Wisconsin 
[Instruction]: 
Your task is analyzing urban heat using Kriging interpolation techniques in Python. The analysis 
should focus on understanding spatial patterns of urban heat islands by using point temperature 
data and interpolating these values across a city. You will have to use a demographic layer to extract 
and enhance the data visualization on the elder group(Age>65). The goal is to load the temperature 
sample data, apply the Kriging method to predict temperature across the urban area, and generate 
a choropleth map showing the average interpolated temperature surface in each census block group. 
Highlighting the area with high interpolated area as well as high density of the elder population. 
The final output should be saved as ""pred_results/interpolated_urban_heat.png"". 
[Domain Knowledge]: 
Kriging is a commonly used spatial interpolation method in soil science and geology. It is based 
on statistical models that include autocorrelation. It involves fitting into a variogram and using 
weights derived from covariance to interpolate. A choropleth map is a thematic map that is used to 
represent statistical data using the color mapping symbology technique. It displays enumeration 
units, or divided geographical areas or regions that are colored, shaded or patterned in relation 
to a data variable. Attribute joins are accomplished using the pandas.merge() method. In general, 
it is recommended to use the merge() method called from the GeoDataFrame in the left argument. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
1,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find heat islands and at-risk populations in Madison, Wisconsin 
[Instruction]: 
Your task is analyzing urban heat using Kriging interpolation techniques in Python. The analysis 
should focus on understanding spatial patterns of urban heat islands by using point temperature 
data and interpolating these values across a city. You will have to use a demographic layer to extract 
and enhance the data visualization on the elder group(Age>65). The goal is to load the temperature 
sample data, apply the Kriging method to predict temperature across the urban area, and generate 
a choropleth map showing the average interpolated temperature surface in each census block group. 
Highlighting the area with high interpolated area as well as high density of the elder population. 
The final output should be saved as ""pred_results/interpolated_urban_heat.png"". 
[Dataset Description]: 
dataset/Temperature.geojson: Geojson file that stores temperature at different coordinates 
as points. 'TemperatureF'' Column stores the temperature information in Fahrenheit. 
Columns of dataset/Temperature.geojson: 
Columns: OBJECTID SID LATITUDE LONGITUDE TemperatureF geometry 

dataset/CensusBlock.geojson: Geojson file that stores the census block of Madison as polygons. 
""Block_Groups_Over65Density"" column stores the density of the elderly population per census 
block. 
Columns of dataset/CensusBlock.geojson: 
OBJECTID, Block_Groups_TOTPOP10, Block_Groups_PopOver65, Block_Groups_Over65Density, 
Shape_Length, Shape_Area, geometry 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
1,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find heat islands and at-risk populations in Madison, Wisconsin 
[Instruction]: 
Your task is analyzing urban heat using Kriging interpolation techniques in Python. The analysis 
should focus on understanding spatial patterns of urban heat islands by using point temperature 
data and interpolating these values across a city. You will have to use a demographic layer to extract 
and enhance the data visualization on the elder group(Age>65). The goal is to load the temperature 
sample data, apply the Kriging method to predict temperature across the urban area, and generate 
a choropleth map showing the average interpolated temperature surface in each census block group. 
Highlighting the area with high interpolated area as well as high density of the elder population. 
The final output should be saved as ""pred_results/interpolated_urban_heat.png"". 
[Domain Knowledge]: 
Kriging is a commonly used spatial interpolation method in soil science and geology. It is based 
on statistical models that include autocorrelation. It involves fitting into a variogram and using 
weights derived from covariance to interpolate. A choropleth map is a thematic map that is used to 
represent statistical data using the color mapping symbology technique. It displays enumeration 
units, or divided geographical areas or regions that are colored, shaded or patterned in relation 
to a data variable. Attribute joins are accomplished using the pandas.merge() method. In general, 
it is recommended to use the merge() method called from the GeoDataFrame in the left argument. 
[Dataset Description]: 
dataset/Temperature.geojson: Geojson file that stores temperature at different coordinates 
as points. 'TemperatureF'' Column stores the temperature information in Fahrenheit. 
Columns of dataset/Temperature.geojson: 
Columns: OBJECTID SID LATITUDE LONGITUDE TemperatureF geometry 

dataset/CensusBlock.geojson: Geojson file that stores the census block of Madison as polygons. 
""Block_Groups_Over65Density"" column stores the density of the elderly population per census 
block. 
Columns of dataset/CensusBlock.geojson: 
OBJECTID, Block_Groups_TOTPOP10, Block_Groups_PopOver65, Block_Groups_Over65Density, 
Shape_Length, Shape_Area, geometry 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
2,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find future bus stop locations in Hamilton, Tennessee 
[Instruction]: 
Your task is performing analysis on public transit access in Hamilton County Tennessee. The goal 
is to map out existing bus stops‘s service area with enriched map layers for different census blocks, 
including poverty, population density, and accessibility to a vehicle. You should overlay the 
three demographical factors with the bus service area using different transparent color schemes. 
Visualizing the resulting data as ""pred_results/transit_access.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
2,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find future bus stop locations in Hamilton, Tennessee 
[Instruction]: 
Your task is performing analysis on public transit access in Hamilton County Tennessee. The goal 
is to map out existing bus stops‘s service area with enriched map layers for different census blocks, 
including poverty, population density, and accessibility to a vehicle. You should overlay the 
three demographical factors with the bus service area using different transparent color schemes. 
Visualizing the resulting data as ""pred_results/transit_access.png"". 
[Domain Knowledge]: 
The Overlay toolset contains tools to overlay multiple feature classes to combine, erase, modify, 
or update spatial features, resulting in a new feature class. The ""overlay"" function manipulations 
two feature layers using the language of sets, intersection, union, difference and symmetrical 
difference. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
2,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find future bus stop locations in Hamilton, Tennessee 
[Instruction]: 
Your task is performing analysis on public transit access in Hamilton County Tennessee. The goal 
is to map out existing bus stops‘s service area with enriched map layers for different census blocks, 
including poverty, population density, and accessibility to a vehicle. You should overlay the 
three demographical factors with the bus service area using different transparent color schemes. 
Visualizing the resulting data as ""pred_results/transit_access.png"". 
[Dataset Description]: 
dataset/BusServiceArea.geojson: Geojson file stores the bus stop service areas in polygons 

Columns of dataset/BusServiceArea.geojson: 
OBJECTID FacilityID ... Shape_Area, geometry 

dataset/HamiltonDemographics.geojson: Geojson file that stores census block demographic information 
in polygons. 'populationToPolygonSizeRating' column represents population density. 'households_ACSHHBPOV' 
represents poverty rate. 'AtRisk_ACSOVEH0' represents population without ownership to vehicles. 


Columns of dataset/HamiltonDemographics.geojson: 
OBJECTID', 'AREALAND', 'AREAWATER', 'BASENAME', 'BLKGRP', 'CENTLAT', 
'CENTLON', 'COUNTY', 'FUNCSTAT', 'GEOID', 'INTPTLAT', 'INTPTLON', 
'LSADC', 'MTFCC', 'NAME', 'ObjID', 'STATE', 'TRACT', 'UR', 'HU100', 
'POP100', 'ALANDHIST', 'AWATERHIST', 'EFFDATE', 'ESTABDATE', 'VINTAGE', 
'STGEOMETRY_AREA', 'STGEOMETRY_LEN', 'aggregationMethod', 'HasData', 
'ORIGINAL_OID', 'sourceCountry', 'apportionmentConfidence', 
'populationToPolygonSizeRating', 'populationtotals_TOTPOP_CY', 
'households_ACSHHBPOV', 'AtRisk_ACSOVEH0', 'Shape_Length', 'Shape_Area', 
'geometry' 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
2,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find future bus stop locations in Hamilton, Tennessee 
[Instruction]: 
Your task is performing analysis on public transit access in Hamilton County Tennessee. The goal 
is to map out existing bus stops‘s service area with enriched map layers for different census blocks, 
including poverty, population density, and accessibility to a vehicle. You should overlay the 
three demographical factors with the bus service area using different transparent color schemes. 
Visualizing the resulting data as ""pred_results/transit_access.png"". 
[Domain Knowledge]: 
The Overlay toolset contains tools to overlay multiple feature classes to combine, erase, modify, 
or update spatial features, resulting in a new feature class. The ""overlay"" function manipulations 
two feature layers using the language of sets, intersection, union, difference and symmetrical 
difference. 
[Dataset Description]: 
dataset/BusServiceArea.geojson: Geojson file stores the bus stop service areas in polygons 

Columns of dataset/BusServiceArea.geojson: 
OBJECTID FacilityID ... Shape_Area, geometry 

dataset/HamiltonDemographics.geojson: Geojson file that stores census block demographic information 
in polygons. 'populationToPolygonSizeRating' column represents population density. 'households_ACSHHBPOV' 
represents poverty rate. 'AtRisk_ACSOVEH0' represents population without ownership to vehicles. 


Columns of dataset/HamiltonDemographics.geojson: 
OBJECTID', 'AREALAND', 'AREAWATER', 'BASENAME', 'BLKGRP', 'CENTLAT', 
'CENTLON', 'COUNTY', 'FUNCSTAT', 'GEOID', 'INTPTLAT', 'INTPTLON', 
'LSADC', 'MTFCC', 'NAME', 'ObjID', 'STATE', 'TRACT', 'UR', 'HU100', 
'POP100', 'ALANDHIST', 'AWATERHIST', 'EFFDATE', 'ESTABDATE', 'VINTAGE', 
'STGEOMETRY_AREA', 'STGEOMETRY_LEN', 'aggregationMethod', 'HasData', 
'ORIGINAL_OID', 'sourceCountry', 'apportionmentConfidence', 
'populationToPolygonSizeRating', 'populationtotals_TOTPOP_CY', 
'households_ACSHHBPOV', 'AtRisk_ACSOVEH0', 'Shape_Length', 'Shape_Area', 
'geometry' 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
3,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Assess burn scars and understanding the impact of wildfire in Montana using satellite imagery 
[Instruction]: 
Your task is assessing burn scars using satellite imagery and perform spatial analysis to understand 
the impact of wildfires. The goal is to use the satellite imagery data from 2014 and 2015 on analyzing 
burn scars by determining the change in Normalized Burn Ratio. Then you should generate a map that 
visualizes the spatial extent of the damage areas in vector data. The final output should be a visual 
representation of the burn scars, saved as ""pred_results/burn_scar_analysis.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
3,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Assess burn scars and understanding the impact of wildfire in Montana using satellite imagery 
[Instruction]: 
Your task is assessing burn scars using satellite imagery and perform spatial analysis to understand 
the impact of wildfires. The goal is to use the satellite imagery data from 2014 and 2015 on analyzing 
burn scars by determining the change in Normalized Burn Ratio. Then you should generate a map that 
visualizes the spatial extent of the damage areas in vector data. The final output should be a visual 
representation of the burn scars, saved as ""pred_results/burn_scar_analysis.png"". 
[Domain Knowledge]: 
Normalized Burn Ratio (NBR) is used to identify burned areas and provide a measure of burn severity. 
NBR ranges between -1 and 1. A high NBR value indicates healthy vegetation. A low value indicates 
bare ground and recently burnt areas.It is calculated as a ratio between the NIR and SWIR values in 
traditional fashion. In Landsat 8-9, NBR = (Band 5 + Band 7) / (Band 5 + Band 7). 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
3,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Assess burn scars and understanding the impact of wildfire in Montana using satellite imagery 
[Instruction]: 
Your task is assessing burn scars using satellite imagery and perform spatial analysis to understand 
the impact of wildfires. The goal is to use the satellite imagery data from 2014 and 2015 on analyzing 
burn scars by determining the change in Normalized Burn Ratio. Then you should generate a map that 
visualizes the spatial extent of the damage areas in vector data. The final output should be a visual 
representation of the burn scars, saved as ""pred_results/burn_scar_analysis.png"". 
[Dataset Description]: 
dataset/G_2014.tif: Raster file of satellite imageries in 2014, taken by Landsat 8 
No preview 
dataset/G_2015.tif: Raster file of satellite imageries in 2015, taken by Landsat 8 
No Preview 

Landsat 8 satellite imageries have 9 bands: 
Band 1 Coastal Aerosol (0.43 - 0.45 µm) 30 m 
Band 2 Blue (0.450 - 0.51 µm) 30 m 
Band 3 Green (0.53 - 0.59 µm) 30 m 
Band 4 Red (0.64 - 0.67 µm) 30 m 
Band 5 Near-Infrared (0.85 - 0.88 µm) 30 m 
Band 6 SWIR 1(1.57 - 1.65 µm) 30 m 
Band 7 SWIR 2 (2.11 - 2.29 µm) 30 m 
Band 8 Panchromatic (PAN) (0.50 - 0.68 µm) 15 m 
Band 9 Cirrus (1.36 - 1.38 µm) 30 m 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
3,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Assess burn scars and understanding the impact of wildfire in Montana using satellite imagery 
[Instruction]: 
Your task is assessing burn scars using satellite imagery and perform spatial analysis to understand 
the impact of wildfires. The goal is to use the satellite imagery data from 2014 and 2015 on analyzing 
burn scars by determining the change in Normalized Burn Ratio. Then you should generate a map that 
visualizes the spatial extent of the damage areas in vector data. The final output should be a visual 
representation of the burn scars, saved as ""pred_results/burn_scar_analysis.png"". 
[Domain Knowledge]: 
Normalized Burn Ratio (NBR) is used to identify burned areas and provide a measure of burn severity. 
NBR ranges between -1 and 1. A high NBR value indicates healthy vegetation. A low value indicates 
bare ground and recently burnt areas.It is calculated as a ratio between the NIR and SWIR values in 
traditional fashion. In Landsat 8-9, NBR = (Band 5 + Band 7) / (Band 5 + Band 7). 
[Dataset Description]: 
dataset/G_2014.tif: Raster file of satellite imageries in 2014, taken by Landsat 8 
No preview 
dataset/G_2015.tif: Raster file of satellite imageries in 2015, taken by Landsat 8 
No Preview 

Landsat 8 satellite imageries have 9 bands: 
Band 1 Coastal Aerosol (0.43 - 0.45 µm) 30 m 
Band 2 Blue (0.450 - 0.51 µm) 30 m 
Band 3 Green (0.53 - 0.59 µm) 30 m 
Band 4 Red (0.64 - 0.67 µm) 30 m 
Band 5 Near-Infrared (0.85 - 0.88 µm) 30 m 
Band 6 SWIR 1(1.57 - 1.65 µm) 30 m 
Band 7 SWIR 2 (2.11 - 2.29 µm) 30 m 
Band 8 Panchromatic (PAN) (0.50 - 0.68 µm) 15 m 
Band 9 Cirrus (1.36 - 1.38 µm) 30 m 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
4,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Identify groundwater vulnerable areas that need protection 
[Instruction]: 
Your task is identifying groundwater vulnerable areas that need protection using the Arcpy. First, 
project 'mc_soils.shp' to Lambert Conformal Conic. Next, using 'mc_boundary.shp' as the extent, 
land_cover as the cellSize and snapRaster, extract the drainage_conditions and water_depth respectively 
from 'mc_soils.shp'. Use drainage_conditions and water_depth to perform suitability modeling 
to calculate vulnerable_areas, and then use vulnerable_areas and land_cover to perform suitability 
modeling to calculate risk_zones. The goal is to use the drainage condition and water depth from 
landcover data to perform suitability modeling for risk areas of ground water. Finally, filter 
out high_risk_zones and undeveloped_areas based on risk_zones, and save the result to 'output/undeveloped_areas.tif'. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
4,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Identify groundwater vulnerable areas that need protection 
[Instruction]: 
Your task is identifying groundwater vulnerable areas that need protection using the Arcpy. First, 
project 'mc_soils.shp' to Lambert Conformal Conic. Next, using 'mc_boundary.shp' as the extent, 
land_cover as the cellSize and snapRaster, extract the drainage_conditions and water_depth respectively 
from 'mc_soils.shp'. Use drainage_conditions and water_depth to perform suitability modeling 
to calculate vulnerable_areas, and then use vulnerable_areas and land_cover to perform suitability 
modeling to calculate risk_zones. The goal is to use the drainage condition and water depth from 
landcover data to perform suitability modeling for risk areas of ground water. Finally, filter 
out high_risk_zones and undeveloped_areas based on risk_zones, and save the result to 'output/undeveloped_areas.tif'. 

[Domain Knowledge]: 
Suitability modeling is an analytical process used to identify the optimal location or suitability 
of geographic areas. It generally consists of three steps: data preparation, data reclassification, 
and weighted overlay. In ArcGIS Pro, suitability modeling is implemented through an integrated 
Modeler, but in Arcpy, it needs to be carried out step by step using basic functions. For preparation: 
Polygon data needs to be converted into Raster before suitability modeling. For reclassification: 
The reclassification of drainage_conditions is [[1, 3], [2, 1], [3, 4], [4, 5], [5, 2]]. The reclassification 
of water_depth is [[0, 10, 5], [10, 33, 4], [33, 61, 3], [61, 80, 2], [80, 92, 1]]. The reclassification 
of vulnerable_areas is standardization to [1, 10]. The reclassification of land_cover is [[11, 
1],[21, 6],[22, 7],[23, 8],[24, 10],[31, 4],[41, 3],[42, 1],[43, 3],[52, 3],[71, 2],[81, 5],[82, 
9],[90, 1],[95, 1]]. For weighted overlay: vulnerable_areas = drainage_conditions * 5 + water_depth 
* 4 risk_zones = vulnerable_areas * 8 + land_cover * 10 For final conditional evaluation: Undeveloped 
areas in the high risk zones (risk_zones > 100) need more protection. Seek for the areas in land_cover 
masking high risk zone, use the following rule: where_clause=""Class IN ('Deciduous Forest', 'Emergent 
Herbaceous Wetlands', 'Hay/Pasture', 'Herbaceous', 'Mixed Forest', 'Shrub/Scrub', 'Woody 
Wetlands', 'Barren Land')"" 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
4,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Identify groundwater vulnerable areas that need protection 
[Instruction]: 
Your task is identifying groundwater vulnerable areas that need protection using the Arcpy. First, 
project 'mc_soils.shp' to Lambert Conformal Conic. Next, using 'mc_boundary.shp' as the extent, 
land_cover as the cellSize and snapRaster, extract the drainage_conditions and water_depth respectively 
from 'mc_soils.shp'. Use drainage_conditions and water_depth to perform suitability modeling 
to calculate vulnerable_areas, and then use vulnerable_areas and land_cover to perform suitability 
modeling to calculate risk_zones. The goal is to use the drainage condition and water depth from 
landcover data to perform suitability modeling for risk areas of ground water. Finally, filter 
out high_risk_zones and undeveloped_areas based on risk_zones, and save the result to 'output/undeveloped_areas.tif'. 

[Dataset Description]: 
dataset/mc_soils.shp: In this shapefile, three fields are important in groundwater analysis: 
Drainage Class – Dominant Conditions, Hydrologic Group – Dominant Conditions, and Water Table 
Depth – Annual – Minimum. Drainage conditions describe the movement of water through soil. It explains 
how water infiltrates into the ground based on the hydrologic group. There are four hydrologic soil 
groups (A, B, C, D) that are defined based on runoff potentials, hydraulic conductivity, and depth 
to any layer. In some instances, soils may be assigned dual hydrologic groups [(A/D, B/D, C/D). Finally, 
the water table depth measures the distance to the water table, usually in centimeters. 
Key Fields: 
'FID', 'Shape', 'AREASYMBOL', 'SPATIALVER', 'MUSYM', 'MUKEY', 'drclassdcd', 'hydgrpdcd', 
'wdepannmin' 

dataset/mc_boundary.shp: This shapefile contains the polygon region of study area. 
Key Fields: 
'FID', 'Shape', 'OBJECTID_1', 'AREA', 'PERIMETER', 'BLMCNTYO_', 'BLMCNTYO_I', 'COUNTY_NAM', 
'COBCODE' 

dataset/land_cover.tif: Raster file showing the land cover of study area, one band. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
4,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Identify groundwater vulnerable areas that need protection 
[Instruction]: 
Your task is identifying groundwater vulnerable areas that need protection using the Arcpy. First, 
project 'mc_soils.shp' to Lambert Conformal Conic. Next, using 'mc_boundary.shp' as the extent, 
land_cover as the cellSize and snapRaster, extract the drainage_conditions and water_depth respectively 
from 'mc_soils.shp'. Use drainage_conditions and water_depth to perform suitability modeling 
to calculate vulnerable_areas, and then use vulnerable_areas and land_cover to perform suitability 
modeling to calculate risk_zones. The goal is to use the drainage condition and water depth from 
landcover data to perform suitability modeling for risk areas of ground water. Finally, filter 
out high_risk_zones and undeveloped_areas based on risk_zones, and save the result to 'output/undeveloped_areas.tif'. 

[Domain Knowledge]: 
Suitability modeling is an analytical process used to identify the optimal location or suitability 
of geographic areas. It generally consists of three steps: data preparation, data reclassification, 
and weighted overlay. In ArcGIS Pro, suitability modeling is implemented through an integrated 
Modeler, but in Arcpy, it needs to be carried out step by step using basic functions. For preparation: 
Polygon data needs to be converted into Raster before suitability modeling. For reclassification: 
The reclassification of drainage_conditions is [[1, 3], [2, 1], [3, 4], [4, 5], [5, 2]]. The reclassification 
of water_depth is [[0, 10, 5], [10, 33, 4], [33, 61, 3], [61, 80, 2], [80, 92, 1]]. The reclassification 
of vulnerable_areas is standardization to [1, 10]. The reclassification of land_cover is [[11, 
1],[21, 6],[22, 7],[23, 8],[24, 10],[31, 4],[41, 3],[42, 1],[43, 3],[52, 3],[71, 2],[81, 5],[82, 
9],[90, 1],[95, 1]]. For weighted overlay: vulnerable_areas = drainage_conditions * 5 + water_depth 
* 4 risk_zones = vulnerable_areas * 8 + land_cover * 10 For final conditional evaluation: Undeveloped 
areas in the high risk zones (risk_zones > 100) need more protection. Seek for the areas in land_cover 
masking high risk zone, use the following rule: where_clause=""Class IN ('Deciduous Forest', 'Emergent 
Herbaceous Wetlands', 'Hay/Pasture', 'Herbaceous', 'Mixed Forest', 'Shrub/Scrub', 'Woody 
Wetlands', 'Barren Land')"" 
[Dataset Description]: 
dataset/mc_soils.shp: In this shapefile, three fields are important in groundwater analysis: 
Drainage Class – Dominant Conditions, Hydrologic Group – Dominant Conditions, and Water Table 
Depth – Annual – Minimum. Drainage conditions describe the movement of water through soil. It explains 
how water infiltrates into the ground based on the hydrologic group. There are four hydrologic soil 
groups (A, B, C, D) that are defined based on runoff potentials, hydraulic conductivity, and depth 
to any layer. In some instances, soils may be assigned dual hydrologic groups [(A/D, B/D, C/D). Finally, 
the water table depth measures the distance to the water table, usually in centimeters. 
Key Fields: 
'FID', 'Shape', 'AREASYMBOL', 'SPATIALVER', 'MUSYM', 'MUKEY', 'drclassdcd', 'hydgrpdcd', 
'wdepannmin' 

dataset/mc_boundary.shp: This shapefile contains the polygon region of study area. 
Key Fields: 
'FID', 'Shape', 'OBJECTID_1', 'AREA', 'PERIMETER', 'BLMCNTYO_', 'BLMCNTYO_I', 'COUNTY_NAM', 
'COBCODE' 

dataset/land_cover.tif: Raster file showing the land cover of study area, one band. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
5,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Visualize the data about children with elevated blood lead levels in a manner that protects individual 
privacy 
[Instruction]: 
Your task is visualizing data about children with elevated blood lead levels in a privacy-conscious 
manner. First, conduct Optimized Hot Spot Analysis to identify clusters without revealing individual 
points. Then, apply tessellation to summarize cases within hexagonal polygons. This approach 
protects sensitive data while clearly representing areas with higher occurrences of elevated 
blood lead levels. The goal is to find the area with potential blood lead rise with specific cases 
saved in point files. The final output, saved as ""pred_results/transit_access.png"", ensures 
stakeholders receive relevant information without accessing specific locations. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
5,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Visualize the data about children with elevated blood lead levels in a manner that protects individual 
privacy 
[Instruction]: 
Your task is visualizing data about children with elevated blood lead levels in a privacy-conscious 
manner. First, conduct Optimized Hot Spot Analysis to identify clusters without revealing individual 
points. Then, apply tessellation to summarize cases within hexagonal polygons. This approach 
protects sensitive data while clearly representing areas with higher occurrences of elevated 
blood lead levels. The goal is to find the area with potential blood lead rise with specific cases 
saved in point files. The final output, saved as ""pred_results/transit_access.png"", ensures 
stakeholders receive relevant information without accessing specific locations. 
[Domain Knowledge]: 
Hot spot analysis is based on mathematical calculations that identify statistically significant 
spatial clusters of high values (hot spots) and low values (cold spots). Tessellation is a method 
of tiling a surface with identical, non-overlapping geometric shapes, like squares, triangles, 
or hexagons. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
5,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Visualize the data about children with elevated blood lead levels in a manner that protects individual 
privacy 
[Instruction]: 
Your task is visualizing data about children with elevated blood lead levels in a privacy-conscious 
manner. First, conduct Optimized Hot Spot Analysis to identify clusters without revealing individual 
points. Then, apply tessellation to summarize cases within hexagonal polygons. This approach 
protects sensitive data while clearly representing areas with higher occurrences of elevated 
blood lead levels. The goal is to find the area with potential blood lead rise with specific cases 
saved in point files. The final output, saved as ""pred_results/transit_access.png"", ensures 
stakeholders receive relevant information without accessing specific locations. 
[Dataset Description]: 
High_Blood_Level_Results.shp: This shapefile contains point data representing incidents of 
elevated blood lead levels in children within the study area. Each point corresponds to a specific 
geographic location where a blood test was conducted, and the results indicated a high concentration 
of lead. 

Columns of High_Blood_Level_Results.shp 
'Address: House', 'Address: Street Name', 'Address: Unit', 'Address: City', 'Address: ZIP', 
'First Name', 'Last Name', 'Bday', 'Address: State', 'Blood Level', 'Blood Level Test Year', 'Race', 
'Gender', 'Age' 

Sacramento_ZIP_Codes.shp: This shapefile contains polygon data representing the boundaries 
of ZIP codes within Sacramento. Each polygon defines the geographic extent of a specific ZIP code. 


Columns of Sacramento_ZIP_Codes.shp: 
'Zip Code', 'Post Office Name', 'State Name', '2021 Total Population', 'Area in square miles', 
'Shape_Length', 'Shape_Area' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
5,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Visualize the data about children with elevated blood lead levels in a manner that protects individual 
privacy 
[Instruction]: 
Your task is visualizing data about children with elevated blood lead levels in a privacy-conscious 
manner. First, conduct Optimized Hot Spot Analysis to identify clusters without revealing individual 
points. Then, apply tessellation to summarize cases within hexagonal polygons. This approach 
protects sensitive data while clearly representing areas with higher occurrences of elevated 
blood lead levels. The goal is to find the area with potential blood lead rise with specific cases 
saved in point files. The final output, saved as ""pred_results/transit_access.png"", ensures 
stakeholders receive relevant information without accessing specific locations. 
[Domain Knowledge]: 
Hot spot analysis is based on mathematical calculations that identify statistically significant 
spatial clusters of high values (hot spots) and low values (cold spots). Tessellation is a method 
of tiling a surface with identical, non-overlapping geometric shapes, like squares, triangles, 
or hexagons. 
[Dataset Description]: 
High_Blood_Level_Results.shp: This shapefile contains point data representing incidents of 
elevated blood lead levels in children within the study area. Each point corresponds to a specific 
geographic location where a blood test was conducted, and the results indicated a high concentration 
of lead. 

Columns of High_Blood_Level_Results.shp 
'Address: House', 'Address: Street Name', 'Address: Unit', 'Address: City', 'Address: ZIP', 
'First Name', 'Last Name', 'Bday', 'Address: State', 'Blood Level', 'Blood Level Test Year', 'Race', 
'Gender', 'Age' 

Sacramento_ZIP_Codes.shp: This shapefile contains polygon data representing the boundaries 
of ZIP codes within Sacramento. Each polygon defines the geographic extent of a specific ZIP code. 


Columns of Sacramento_ZIP_Codes.shp: 
'Zip Code', 'Post Office Name', 'State Name', '2021 Total Population', 'Area in square miles', 
'Shape_Length', 'Shape_Area' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
6,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Use animal GPS tracks to model home range to understand where they are and how they move over time. 

[Instruction]: 
Yout task is analyzing and visualizing Elk movements in the given dataset. The goal is to estimate 
home ranges and assess habitat preferences using spatial analysis techniques, including Minimum 
Bounding Geometry (Convex Hull), Kernel Density, and Density-based Clustering(DBSCAN). You 
should identify the spatial clusters of Elk movements and overlay the findings with maps and visualizations. 
Save the final output as ""pred_results/Elk_Analysis.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
6,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Use animal GPS tracks to model home range to understand where they are and how they move over time. 

[Instruction]: 
Yout task is analyzing and visualizing Elk movements in the given dataset. The goal is to estimate 
home ranges and assess habitat preferences using spatial analysis techniques, including Minimum 
Bounding Geometry (Convex Hull), Kernel Density, and Density-based Clustering(DBSCAN). You 
should identify the spatial clusters of Elk movements and overlay the findings with maps and visualizations. 
Save the final output as ""pred_results/Elk_Analysis.png"". 
[Domain Knowledge]: 
""Home range"" can be defined as the area within which an animal normally lives and finds what it needs 
for survival. Basically, the home range is the area that an animal travels for its normal daily activities. 
""Minimum Bounding Geometry"" creates a feature class containing polygons which represent a specified 
minimum bounding geometry enclosing each input feature or each group of input features. ""Convex 
hull"" is the smallest convex polygon that can enclose a group of objects, such as a group of points. 
""Kernel Density Mapping"" calculates and visualizes features's density in a given area. ""DBSCAN"", 
Density-Based Spatial Clustering of Applications with Noise that cluster the points based on density 
criterion. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
6,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Use animal GPS tracks to model home range to understand where they are and how they move over time. 

[Instruction]: 
Yout task is analyzing and visualizing Elk movements in the given dataset. The goal is to estimate 
home ranges and assess habitat preferences using spatial analysis techniques, including Minimum 
Bounding Geometry (Convex Hull), Kernel Density, and Density-based Clustering(DBSCAN). You 
should identify the spatial clusters of Elk movements and overlay the findings with maps and visualizations. 
Save the final output as ""pred_results/Elk_Analysis.png"". 
[Dataset Description]: 
dataset/Elk_in_Southwestern_Alberta_2009.geojson: geojson files for storing points of Elk 
movements in Southwestern Alberta 2009. 

Columns of dataset/Elk_in_Southwestern_Alberta_2009.geojson: 
'OBJECTID', 'timestamp', 'long', 'lat', 'comments', 'external_t', 'dop', 
'fix_type_r', 'satellite_', 'height', 'crc_status', 'outlier_ma', 
'sensor_typ', 'individual', 'tag_ident', 'ind_ident', 'study_name', 
'date', 'time', 'timestamp_Converted', 'summer_indicator', 'geometry' 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
6,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Use animal GPS tracks to model home range to understand where they are and how they move over time. 

[Instruction]: 
Yout task is analyzing and visualizing Elk movements in the given dataset. The goal is to estimate 
home ranges and assess habitat preferences using spatial analysis techniques, including Minimum 
Bounding Geometry (Convex Hull), Kernel Density, and Density-based Clustering(DBSCAN). You 
should identify the spatial clusters of Elk movements and overlay the findings with maps and visualizations. 
Save the final output as ""pred_results/Elk_Analysis.png"". 
[Domain Knowledge]: 
""Home range"" can be defined as the area within which an animal normally lives and finds what it needs 
for survival. Basically, the home range is the area that an animal travels for its normal daily activities. 
""Minimum Bounding Geometry"" creates a feature class containing polygons which represent a specified 
minimum bounding geometry enclosing each input feature or each group of input features. ""Convex 
hull"" is the smallest convex polygon that can enclose a group of objects, such as a group of points. 
""Kernel Density Mapping"" calculates and visualizes features's density in a given area. ""DBSCAN"", 
Density-Based Spatial Clustering of Applications with Noise that cluster the points based on density 
criterion. 
[Dataset Description]: 
dataset/Elk_in_Southwestern_Alberta_2009.geojson: geojson files for storing points of Elk 
movements in Southwestern Alberta 2009. 

Columns of dataset/Elk_in_Southwestern_Alberta_2009.geojson: 
'OBJECTID', 'timestamp', 'long', 'lat', 'comments', 'external_t', 'dop', 
'fix_type_r', 'satellite_', 'height', 'crc_status', 'outlier_ma', 
'sensor_typ', 'individual', 'tag_ident', 'ind_ident', 'study_name', 
'date', 'time', 'timestamp_Converted', 'summer_indicator', 'geometry' 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
7,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the impacts of land subsidence on flooding 
[Instruction]: 
Your task is analyzing the impact of land subsidence on flooding based on future elevation data of 
the study area. Identify flood-prone areas and estimate potential building damage to support urban 
planning and mitigation strategies. Save the results to ""pred_results/flooding_analysis.png"". 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
7,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the impacts of land subsidence on flooding 
[Instruction]: 
Your task is analyzing the impact of land subsidence on flooding based on future elevation data of 
the study area. Identify flood-prone areas and estimate potential building damage to support urban 
planning and mitigation strategies. Save the results to ""pred_results/flooding_analysis.png"". 

[Domain Knowledge]: 
Estimate potential building damage based on factors such as flood depth, building type, flood risk 
classification and economic cost/loss. The cost calculation can be done by:(0.298 * (Log(0.01 
* mean depth)) + 1.4502) * 271 * shape area Raster calculation involves creating a algebra expression 
that outputs a raster file. The Overlay toolset contains tools to overlay multiple feature classes 
to combine, erase, modify, or update spatial features, resulting in a new feature class. The ""overlay"" 
function manipulations two feature layers using the language of sets, intersection, union, difference 
and symmetrical difference. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
7,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the impacts of land subsidence on flooding 
[Instruction]: 
Your task is analyzing the impact of land subsidence on flooding based on future elevation data of 
the study area. Identify flood-prone areas and estimate potential building damage to support urban 
planning and mitigation strategies. Save the results to ""pred_results/flooding_analysis.png"". 

[Dataset Description]: 
dataset/Elevation_2050.tif: Raster file for storing predicted flood elevation in 2050. 
data shape: (878, 1196) 

dataset/StudyAreaBuildings.shp: Polygon Shapefile for shape and features of buildings in the 
study area. 

Columns of dataset/StudyAreaBuildings.shp: 
'Shape_Leng', 'Shape_Area', 'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
7,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the impacts of land subsidence on flooding 
[Instruction]: 
Your task is analyzing the impact of land subsidence on flooding based on future elevation data of 
the study area. Identify flood-prone areas and estimate potential building damage to support urban 
planning and mitigation strategies. Save the results to ""pred_results/flooding_analysis.png"". 

[Domain Knowledge]: 
Estimate potential building damage based on factors such as flood depth, building type, flood risk 
classification and economic cost/loss. The cost calculation can be done by:(0.298 * (Log(0.01 
* mean depth)) + 1.4502) * 271 * shape area Raster calculation involves creating a algebra expression 
that outputs a raster file. The Overlay toolset contains tools to overlay multiple feature classes 
to combine, erase, modify, or update spatial features, resulting in a new feature class. The ""overlay"" 
function manipulations two feature layers using the language of sets, intersection, union, difference 
and symmetrical difference. 
[Dataset Description]: 
dataset/Elevation_2050.tif: Raster file for storing predicted flood elevation in 2050. 
data shape: (878, 1196) 

dataset/StudyAreaBuildings.shp: Polygon Shapefile for shape and features of buildings in the 
study area. 

Columns of dataset/StudyAreaBuildings.shp: 
'Shape_Leng', 'Shape_Area', 'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
8,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find gap for Toronto fire station service coverage. 
[Instruction]: 
Your task is analyzing Toronto fire stations and their service coverage. The goal is visualize the 
current fire station coverage in Etobicoke by buffering to identify coverage gaps. Save the figure 
as ""pred_results/Fire_Service_Analysis.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
8,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find gap for Toronto fire station service coverage. 
[Instruction]: 
Your task is analyzing Toronto fire stations and their service coverage. The goal is visualize the 
current fire station coverage in Etobicoke by buffering to identify coverage gaps. Save the figure 
as ""pred_results/Fire_Service_Analysis.png"". 
[Domain Knowledge]: 
""Service Area"" or ""Service Coverage"" is a region that encompasses all accessible locations within 
the study area. It is usually used to identify how much land, how many people, or how much of anything 
else is within the neighborhood or a region and served by certain facilities. The Overlay toolset 
contains tools to overlay multiple feature classes to combine, erase, modify, or update spatial 
features, resulting in a new feature class. The ""overlay"" function manipulations two feature layers 
using the language of sets, intersection, union, difference and symmetrical difference. ""Buffer"" 
creates buffer polygons or zones around input geometry features to a specified distance in GIS. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
8,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find gap for Toronto fire station service coverage. 
[Instruction]: 
Your task is analyzing Toronto fire stations and their service coverage. The goal is visualize the 
current fire station coverage in Etobicoke by buffering to identify coverage gaps. Save the figure 
as ""pred_results/Fire_Service_Analysis.png"". 
[Dataset Description]: 
dataset/etobicoke.geojson: Polygon Geojson file for neighborhood region for Etobicoke. 

Columns of dataset/etobicoke.geojson 
'OBJECTID_1', 'LCODE_NAME', 'SECTION', 'DISTRICT', 'DIVISION', 
'OBJECTID', 'Shape_Length', 'Shape_Area', 'geometry' 

dataset/fire_stations.geojson: Point Geojson file for current fire stations distribution as 
points. 

Columns of dataset/fire_stations.geojson: 
'OBJECTID_1', 'NAME', 'ADDRESS', 'WARD_NAME', 'MUN_NAME', 'geometry 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
8,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find gap for Toronto fire station service coverage. 
[Instruction]: 
Your task is analyzing Toronto fire stations and their service coverage. The goal is visualize the 
current fire station coverage in Etobicoke by buffering to identify coverage gaps. Save the figure 
as ""pred_results/Fire_Service_Analysis.png"". 
[Domain Knowledge]: 
""Service Area"" or ""Service Coverage"" is a region that encompasses all accessible locations within 
the study area. It is usually used to identify how much land, how many people, or how much of anything 
else is within the neighborhood or a region and served by certain facilities. The Overlay toolset 
contains tools to overlay multiple feature classes to combine, erase, modify, or update spatial 
features, resulting in a new feature class. The ""overlay"" function manipulations two feature layers 
using the language of sets, intersection, union, difference and symmetrical difference. ""Buffer"" 
creates buffer polygons or zones around input geometry features to a specified distance in GIS. 

[Dataset Description]: 
dataset/etobicoke.geojson: Polygon Geojson file for neighborhood region for Etobicoke. 

Columns of dataset/etobicoke.geojson 
'OBJECTID_1', 'LCODE_NAME', 'SECTION', 'DISTRICT', 'DIVISION', 
'OBJECTID', 'Shape_Length', 'Shape_Area', 'geometry' 

dataset/fire_stations.geojson: Point Geojson file for current fire stations distribution as 
points. 

Columns of dataset/fire_stations.geojson: 
'OBJECTID_1', 'NAME', 'ADDRESS', 'WARD_NAME', 'MUN_NAME', 'geometry 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
9,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find the deforestation rate for Rondônia 
[Instruction]: 
Your task is finding the deforestation rate for Brazilian state of Rondônia. The goal is calculating 
the deforestation area percentage in the Rondônia within the buffer zone of 5.5km around road layers. 
Save the percentage result in a CSV file named ""pred_results/deforestation_rate.csv"" with a column 
title percentage_deforestation. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
9,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find the deforestation rate for Rondônia 
[Instruction]: 
Your task is finding the deforestation rate for Brazilian state of Rondônia. The goal is calculating 
the deforestation area percentage in the Rondônia within the buffer zone of 5.5km around road layers. 
Save the percentage result in a CSV file named ""pred_results/deforestation_rate.csv"" with a column 
title percentage_deforestation. 
[Domain Knowledge]: 
""Buffer"" creates buffer polygons or zones around input geometry features to a specified distance 
in GIS. Clip points, lines, or polygon geometries to the mask extent. ""Clip"" is used to cut out a piece 
of one map layer using one or more of the features in another layer. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
9,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find the deforestation rate for Rondônia 
[Instruction]: 
Your task is finding the deforestation rate for Brazilian state of Rondônia. The goal is calculating 
the deforestation area percentage in the Rondônia within the buffer zone of 5.5km around road layers. 
Save the percentage result in a CSV file named ""pred_results/deforestation_rate.csv"" with a column 
title percentage_deforestation. 
[Dataset Description]: 
dataset/roads.geojson: Linestring Geojson file storing the current road map in Brazilian state 
of Rondônia. 
Status columns include values such as ""Official"", ""Unofficial"". 
Column of dataset/roads.geojson: 
OBJECTID Name Status Shape_Length geometry 


dataset/deforestedArea.geojson: Polygon Geojson file include the entire deforested area as 
one row of polygon. 

Columns of dataset/deforestedArea.geojson: 
OBJECTID Shape_Length Shape_Area geometry 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
9,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find the deforestation rate for Rondônia 
[Instruction]: 
Your task is finding the deforestation rate for Brazilian state of Rondônia. The goal is calculating 
the deforestation area percentage in the Rondônia within the buffer zone of 5.5km around road layers. 
Save the percentage result in a CSV file named ""pred_results/deforestation_rate.csv"" with a column 
title percentage_deforestation. 
[Domain Knowledge]: 
""Buffer"" creates buffer polygons or zones around input geometry features to a specified distance 
in GIS. Clip points, lines, or polygon geometries to the mask extent. ""Clip"" is used to cut out a piece 
of one map layer using one or more of the features in another layer. 
[Dataset Description]: 
dataset/roads.geojson: Linestring Geojson file storing the current road map in Brazilian state 
of Rondônia. 
Status columns include values such as ""Official"", ""Unofficial"". 
Column of dataset/roads.geojson: 
OBJECTID Name Status Shape_Length geometry 


dataset/deforestedArea.geojson: Polygon Geojson file include the entire deforested area as 
one row of polygon. 

Columns of dataset/deforestedArea.geojson: 
OBJECTID Shape_Length Shape_Area geometry 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
10,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the potential impact of proposed roads on local environment 
[Instruction]: 
Your task is predicting the effect of future proposed roads on the deforestation of in the Brazilian 
state of Rondônia. A planned road network will be added to Rondônia, create a buffer zone of 5.5km 
around the new roads. Visualize the change created by the roads with the planned roads, existing 
roads, deforested area, and protected forest along with the boundary of Rondônia. Save the output 
as ""pred_results/predictedRiskyArea.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
10,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the potential impact of proposed roads on local environment 
[Instruction]: 
Your task is predicting the effect of future proposed roads on the deforestation of in the Brazilian 
state of Rondônia. A planned road network will be added to Rondônia, create a buffer zone of 5.5km 
around the new roads. Visualize the change created by the roads with the planned roads, existing 
roads, deforested area, and protected forest along with the boundary of Rondônia. Save the output 
as ""pred_results/predictedRiskyArea.png"". 
[Domain Knowledge]: 
""Buffer"" creates buffer polygons or zones around input geometry features to a specified distance. 
Clip points, lines, or polygon geometries to the mask extent. ""Clip"" is used to cut out a piece of one 
map layer using one or more of the features in another layer. The Overlay toolset contains tools to 
overlay multiple feature classes to combine, erase, modify, or update spatial features, resulting 
in a new feature class. The ""overlay"" function manipulations two feature layers using the language 
of sets 鈥 intersection, union, difference and symmetrical difference. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
10,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the potential impact of proposed roads on local environment 
[Instruction]: 
Your task is predicting the effect of future proposed roads on the deforestation of in the Brazilian 
state of Rondônia. A planned road network will be added to Rondônia, create a buffer zone of 5.5km 
around the new roads. Visualize the change created by the roads with the planned roads, existing 
roads, deforested area, and protected forest along with the boundary of Rondônia. Save the output 
as ""pred_results/predictedRiskyArea.png"". 
[Dataset Description]: 
dataset/roads.geojson: Linestring Geojson file storing the current road map in Brazilian state 
of Rondônia. 
Status columns include values such as ""Official"", ""Unofficial"". 
Column of dataset/roads.geojson: 
OBJECTID Name Status Shape_Length geometry 

dataset/deforestedArea.geojson: Polygon Geojson file include the entire deforested area as 
one row of polygon. 

Columns of dataset/deforestedArea.geojson: 
OBJECTID Shape_Length Shape_Area geometry 

dataset/protectedForest.geojson: Polygon Geojson file storing mutiple natural reservation 
areas. 
Columns of dataset/protectedForest.geojson: 
OBJECTID_1 Name Category Jurisdiction Created Shape_Length Shape_Area geometry 

dataset/planned_road.geojson: One row of proposed road in the future urban planning process, 
saved in linestring Geojson format/ 
Columns of dataset/planned_road.geojson 
OBJECTID Shape_Length Name Status geometry 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
10,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the potential impact of proposed roads on local environment 
[Instruction]: 
Your task is predicting the effect of future proposed roads on the deforestation of in the Brazilian 
state of Rondônia. A planned road network will be added to Rondônia, create a buffer zone of 5.5km 
around the new roads. Visualize the change created by the roads with the planned roads, existing 
roads, deforested area, and protected forest along with the boundary of Rondônia. Save the output 
as ""pred_results/predictedRiskyArea.png"". 
[Domain Knowledge]: 
""Buffer"" creates buffer polygons or zones around input geometry features to a specified distance. 
Clip points, lines, or polygon geometries to the mask extent. ""Clip"" is used to cut out a piece of one 
map layer using one or more of the features in another layer. The Overlay toolset contains tools to 
overlay multiple feature classes to combine, erase, modify, or update spatial features, resulting 
in a new feature class. The ""overlay"" function manipulations two feature layers using the language 
of sets 鈥 intersection, union, difference and symmetrical difference. 
[Dataset Description]: 
dataset/roads.geojson: Linestring Geojson file storing the current road map in Brazilian state 
of Rondônia. 
Status columns include values such as ""Official"", ""Unofficial"". 
Column of dataset/roads.geojson: 
OBJECTID Name Status Shape_Length geometry 

dataset/deforestedArea.geojson: Polygon Geojson file include the entire deforested area as 
one row of polygon. 

Columns of dataset/deforestedArea.geojson: 
OBJECTID Shape_Length Shape_Area geometry 

dataset/protectedForest.geojson: Polygon Geojson file storing mutiple natural reservation 
areas. 
Columns of dataset/protectedForest.geojson: 
OBJECTID_1 Name Category Jurisdiction Created Shape_Length Shape_Area geometry 

dataset/planned_road.geojson: One row of proposed road in the future urban planning process, 
saved in linestring Geojson format/ 
Columns of dataset/planned_road.geojson 
OBJECTID Shape_Length Name Status geometry 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
11,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Create charts using Python to better explore the distribution of corals and sponges around Catalina 
Island. 
[Instruction]: 
Your task is performing raster analysis on the elevation and environmental factors influencing 
coral and sponge distribution at Catalina Island. Update the database with the slope and aspect 
analysis results. Use the updated database to visualize the mean slope and aspect distribution 
of each Coral and Sponge species in Catalina Island as ""pred_results/CoralandSponge.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
11,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Create charts using Python to better explore the distribution of corals and sponges around Catalina 
Island. 
[Instruction]: 
Your task is performing raster analysis on the elevation and environmental factors influencing 
coral and sponge distribution at Catalina Island. Update the database with the slope and aspect 
analysis results. Use the updated database to visualize the mean slope and aspect distribution 
of each Coral and Sponge species in Catalina Island as ""pred_results/CoralandSponge.png"". 
[Domain Knowledge]: 
The Slope tool identifies the steepness at each cell of a raster surface. The lower the slope value, 
the flatter the terrain; the higher the slope value, the steeper the terrain. The Aspect tool identifies 
the direction the downhill slope faces. The values of each cell in the output raster indicate the 
compass direction the surface faces at that location. It is measured clockwise in degrees from 0 
(due north) to 360 (again due north), coming full circle. Flat areas having no downslope direction 
are given a value of -1. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
11,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Create charts using Python to better explore the distribution of corals and sponges around Catalina 
Island. 
[Instruction]: 
Your task is performing raster analysis on the elevation and environmental factors influencing 
coral and sponge distribution at Catalina Island. Update the database with the slope and aspect 
analysis results. Use the updated database to visualize the mean slope and aspect distribution 
of each Coral and Sponge species in Catalina Island as ""pred_results/CoralandSponge.png"". 
[Dataset Description]: 
dataset/CatalinaBathymetry.tif: Raster files for storing Bathymetry data in the Catalina Island 
area. 
Shape of dataset/CatalinaBathymetry.tif:(664, 853) 

dataset/CoralandSpongeCatalina.geojson: Geojson file for storing Coral and Sponge species 
distributions as point along with their attributes. 

Columns of dataset/CoralandSpongeCatalina.geojson: 
'OBJECTID', 'ShallowFlag', 'DatasetID', 'CatalogNumber', 'SampleID', 
'Repository', 'ScientificName', 'VernacularNameCategory', 'TaxonRank', 
'IdentificationQualifier', 'Locality', 'latitude', 'longitude', 
'DepthInMeters', 'DepthMethod', 'ObservationDate', 'SurveyID', 
'Station', 'EventID', 'SamplingEquipment', 'LocationAccuracy', 
'RecordType', 'DataProvider', 'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
11,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Create charts using Python to better explore the distribution of corals and sponges around Catalina 
Island. 
[Instruction]: 
Your task is performing raster analysis on the elevation and environmental factors influencing 
coral and sponge distribution at Catalina Island. Update the database with the slope and aspect 
analysis results. Use the updated database to visualize the mean slope and aspect distribution 
of each Coral and Sponge species in Catalina Island as ""pred_results/CoralandSponge.png"". 
[Domain Knowledge]: 
The Slope tool identifies the steepness at each cell of a raster surface. The lower the slope value, 
the flatter the terrain; the higher the slope value, the steeper the terrain. The Aspect tool identifies 
the direction the downhill slope faces. The values of each cell in the output raster indicate the 
compass direction the surface faces at that location. It is measured clockwise in degrees from 0 
(due north) to 360 (again due north), coming full circle. Flat areas having no downslope direction 
are given a value of -1. 
[Dataset Description]: 
dataset/CatalinaBathymetry.tif: Raster files for storing Bathymetry data in the Catalina Island 
area. 
Shape of dataset/CatalinaBathymetry.tif:(664, 853) 

dataset/CoralandSpongeCatalina.geojson: Geojson file for storing Coral and Sponge species 
distributions as point along with their attributes. 

Columns of dataset/CoralandSpongeCatalina.geojson: 
'OBJECTID', 'ShallowFlag', 'DatasetID', 'CatalogNumber', 'SampleID', 
'Repository', 'ScientificName', 'VernacularNameCategory', 'TaxonRank', 
'IdentificationQualifier', 'Locality', 'latitude', 'longitude', 
'DepthInMeters', 'DepthMethod', 'ObservationDate', 'SurveyID', 
'Station', 'EventID', 'SamplingEquipment', 'LocationAccuracy', 
'RecordType', 'DataProvider', 'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
12,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is evaluating mountain lion habitat suitability, and the first step is calculating terrain 
ruggedness using elevation data. Visualizing the ruggedness and save the outputs as ""pred_results/ruggedness.png"". 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
12,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is evaluating mountain lion habitat suitability, and the first step is calculating terrain 
ruggedness using elevation data. Visualizing the ruggedness and save the outputs as ""pred_results/ruggedness.png"". 

[Domain Knowledge]: 
Terrain Ruggedness Index (TRI) expresses the amount of elevation difference between adjacent 
cells of a DEM. The TRI function measures the difference in elevation values from a center cell and 
eight cells directly surrounding it. Then, the eight elevation differences are squared and averaged. 
The square root of this average results is a TRI measurement for the center cell. This calculation 
is then conducted on every cell of the DEM. The ""generic_filter"" function imported from the ""scipy.ndimage"" 
package can calculate a multidimensional filter using a given ""filter_range"" function. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
12,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is evaluating mountain lion habitat suitability, and the first step is calculating terrain 
ruggedness using elevation data. Visualizing the ruggedness and save the outputs as ""pred_results/ruggedness.png"". 

[Dataset Description]: 
dataset/Elevation.tif: Raster files for storing elevation data. 
Shape of dataset/Elevation.tif: (2494, 3062) 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
12,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is evaluating mountain lion habitat suitability, and the first step is calculating terrain 
ruggedness using elevation data. Visualizing the ruggedness and save the outputs as ""pred_results/ruggedness.png"". 

[Domain Knowledge]: 
Terrain Ruggedness Index (TRI) expresses the amount of elevation difference between adjacent 
cells of a DEM. The TRI function measures the difference in elevation values from a center cell and 
eight cells directly surrounding it. Then, the eight elevation differences are squared and averaged. 
The square root of this average results is a TRI measurement for the center cell. This calculation 
is then conducted on every cell of the DEM. The ""generic_filter"" function imported from the ""scipy.ndimage"" 
package can calculate a multidimensional filter using a given ""filter_range"" function. 
[Dataset Description]: 
dataset/Elevation.tif: Raster files for storing elevation data. 
Shape of dataset/Elevation.tif: (2494, 3062) 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
13,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Understand the relationship between ocean temperature and salinity at various depths in the South 
Atlantic Ocean. 
[Instruction]: 
Your task is using sea temperature and salinity data from the South Atlantic Ocean to calculate and 
analyze a vertical temperature-salinity profile within a specified range of latitude, longitude, 
and depth. Plot the profile and save the figure as 'pred_results/ocean_profiles_vis.png'. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
13,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Understand the relationship between ocean temperature and salinity at various depths in the South 
Atlantic Ocean. 
[Instruction]: 
Your task is using sea temperature and salinity data from the South Atlantic Ocean to calculate and 
analyze a vertical temperature-salinity profile within a specified range of latitude, longitude, 
and depth. Plot the profile and save the figure as 'pred_results/ocean_profiles_vis.png'. 
[Domain Knowledge]: 
iris.plot is a Iris-specific package extensions to matplotlib and for handling multi-dimensional 
data and associated metadata. Cubes can be pattern matched and filtered according to specific criteria. 
Constraints are the mechanism by which cubes can be pattern matched and filtered according to specific 
criteria. Once a constraint has been defined, it can be applied to cubes using the Constraint.extract() 
method. The iris.load() function automatically recognises the format of the given files and attempts 
to produce Iris multidimensional Cubes from their contents. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
13,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Understand the relationship between ocean temperature and salinity at various depths in the South 
Atlantic Ocean. 
[Instruction]: 
Your task is using sea temperature and salinity data from the South Atlantic Ocean to calculate and 
analyze a vertical temperature-salinity profile within a specified range of latitude, longitude, 
and depth. Plot the profile and save the figure as 'pred_results/ocean_profiles_vis.png'. 
[Dataset Description]: 
dataset/atlantic_profiles.nc: NetCDF profile data of partial Atlantic ocean with temperature 
and salinity data. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
13,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Understand the relationship between ocean temperature and salinity at various depths in the South 
Atlantic Ocean. 
[Instruction]: 
Your task is using sea temperature and salinity data from the South Atlantic Ocean to calculate and 
analyze a vertical temperature-salinity profile within a specified range of latitude, longitude, 
and depth. Plot the profile and save the figure as 'pred_results/ocean_profiles_vis.png'. 
[Domain Knowledge]: 
iris.plot is a Iris-specific package extensions to matplotlib and for handling multi-dimensional 
data and associated metadata. Cubes can be pattern matched and filtered according to specific criteria. 
Constraints are the mechanism by which cubes can be pattern matched and filtered according to specific 
criteria. Once a constraint has been defined, it can be applied to cubes using the Constraint.extract() 
method. The iris.load() function automatically recognises the format of the given files and attempts 
to produce Iris multidimensional Cubes from their contents. 
[Dataset Description]: 
dataset/atlantic_profiles.nc: NetCDF profile data of partial Atlantic ocean with temperature 
and salinity data. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
14,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Detecting persistent periods of high temperature over time in past 240 years. 
[Instruction]: 
Your task is using 240 years of annual average surface temperature data from North America to calculate 
and plot the number of occurrences where the temperature exceeds 280K for five consecutive years. 
Save the figure as 'pred_results/temperature_statistic_vis.png'. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
14,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Detecting persistent periods of high temperature over time in past 240 years. 
[Instruction]: 
Your task is using 240 years of annual average surface temperature data from North America to calculate 
and plot the number of occurrences where the temperature exceeds 280K for five consecutive years. 
Save the figure as 'pred_results/temperature_statistic_vis.png'. 
[Domain Knowledge]: 
iris.plot is a Iris-specific package extensions to matplotlib and for handling multi-dimensional 
data and associated metadata. Cubes can be pattern matched and filtered according to specific criteria. 
Constraints are the mechanism by which cubes can be pattern matched and filtered according to specific 
criteria. Once a constraint has been defined, it can be applied to cubes using the Constraint.extract() 
method. The iris.load() function automatically recognises the format of the given files and attempts 
to produce Iris multidimensional Cubes from their contents. cube.collapsed () function can collapse 
one or more dimensions over the cube given the coordinate/s and an aggregation. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
14,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Detecting persistent periods of high temperature over time in past 240 years. 
[Instruction]: 
Your task is using 240 years of annual average surface temperature data from North America to calculate 
and plot the number of occurrences where the temperature exceeds 280K for five consecutive years. 
Save the figure as 'pred_results/temperature_statistic_vis.png'. 
[Dataset Description]: 
dataset/E1_north_america.nc: NetCDF Temperature data in North America for a 240 year period. 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
14,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Detecting persistent periods of high temperature over time in past 240 years. 
[Instruction]: 
Your task is using 240 years of annual average surface temperature data from North America to calculate 
and plot the number of occurrences where the temperature exceeds 280K for five consecutive years. 
Save the figure as 'pred_results/temperature_statistic_vis.png'. 
[Domain Knowledge]: 
iris.plot is a Iris-specific package extensions to matplotlib and for handling multi-dimensional 
data and associated metadata. Cubes can be pattern matched and filtered according to specific criteria. 
Constraints are the mechanism by which cubes can be pattern matched and filtered according to specific 
criteria. Once a constraint has been defined, it can be applied to cubes using the Constraint.extract() 
method. The iris.load() function automatically recognises the format of the given files and attempts 
to produce Iris multidimensional Cubes from their contents. cube.collapsed () function can collapse 
one or more dimensions over the cube given the coordinate/s and an aggregation. 
[Dataset Description]: 
dataset/E1_north_america.nc: NetCDF Temperature data in North America for a 240 year period. 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
15,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Understand the geographical distribution of Total Electron Content (TEC) in the ionosphere. 
[Instruction]: 
Your task is analyzing the distribution of Total Electron Content. Load Total Electron Content 
(TEC) data from a space weather NetCDF file and visualize it as a filled contour map with a geographical 
and coastline background. Save the figure as 'pred_results/TEC_vis.png'. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
15,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Understand the geographical distribution of Total Electron Content (TEC) in the ionosphere. 
[Instruction]: 
Your task is analyzing the distribution of Total Electron Content. Load Total Electron Content 
(TEC) data from a space weather NetCDF file and visualize it as a filled contour map with a geographical 
and coastline background. Save the figure as 'pred_results/TEC_vis.png'. 
[Domain Knowledge]: 
matplotlib.pyplot.contour() draw contour lines while matplotlib.pyplot.contourf() draw filled 
contours. NetCDF (Network Common Data Form) files are commonly used to store multi-dimensional 
scientific data such as temperature, humidity, wind speed, etc. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
15,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Understand the geographical distribution of Total Electron Content (TEC) in the ionosphere. 
[Instruction]: 
Your task is analyzing the distribution of Total Electron Content. Load Total Electron Content 
(TEC) data from a space weather NetCDF file and visualize it as a filled contour map with a geographical 
and coastline background. Save the figure as 'pred_results/TEC_vis.png'. 
[Dataset Description]: 
dataset/space_weather.nc: Total ELectron Content data in netCDF format. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
15,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Understand the geographical distribution of Total Electron Content (TEC) in the ionosphere. 
[Instruction]: 
Your task is analyzing the distribution of Total Electron Content. Load Total Electron Content 
(TEC) data from a space weather NetCDF file and visualize it as a filled contour map with a geographical 
and coastline background. Save the figure as 'pred_results/TEC_vis.png'. 
[Domain Knowledge]: 
matplotlib.pyplot.contour() draw contour lines while matplotlib.pyplot.contourf() draw filled 
contours. NetCDF (Network Common Data Form) files are commonly used to store multi-dimensional 
scientific data such as temperature, humidity, wind speed, etc. 
[Dataset Description]: 
dataset/space_weather.nc: Total ELectron Content data in netCDF format. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
16,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze climate change trends in North America using spatial temporal data 
[Instruction]: 
Load North America climate data in NetCDF file and extract temperature data along the time series, 
then perform a quadratic polynomial fit analysis on the temperature data, and output the fitting 
results by year in 'pred_results/polynomial_fit_pred.csv'. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
16,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze climate change trends in North America using spatial temporal data 
[Instruction]: 
Load North America climate data in NetCDF file and extract temperature data along the time series, 
then perform a quadratic polynomial fit analysis on the temperature data, and output the fitting 
results by year in 'pred_results/polynomial_fit_pred.csv'. 
[Domain Knowledge]: 
NetCDF (Network Common Data Form) files are commonly used to store multi-dimensional scientific 
data such as temperature, humidity, wind speed, etc. numpy.polyfit() is a least squares polynomial 
fitting function. numpy.polyval() estimates polynomial Y values at specific input X values. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
16,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze climate change trends in North America using spatial temporal data 
[Instruction]: 
Load North America climate data in NetCDF file and extract temperature data along the time series, 
then perform a quadratic polynomial fit analysis on the temperature data, and output the fitting 
results by year in 'pred_results/polynomial_fit_pred.csv'. 
[Dataset Description]: 
dataset/A1B_north_america.nc: NetCDF data of climate data and temperature data in North America. 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
16,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze climate change trends in North America using spatial temporal data 
[Instruction]: 
Load North America climate data in NetCDF file and extract temperature data along the time series, 
then perform a quadratic polynomial fit analysis on the temperature data, and output the fitting 
results by year in 'pred_results/polynomial_fit_pred.csv'. 
[Domain Knowledge]: 
NetCDF (Network Common Data Form) files are commonly used to store multi-dimensional scientific 
data such as temperature, humidity, wind speed, etc. numpy.polyfit() is a least squares polynomial 
fitting function. numpy.polyval() estimates polynomial Y values at specific input X values. 
[Dataset Description]: 
dataset/A1B_north_america.nc: NetCDF data of climate data and temperature data in North America. 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
17,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the geographical distribution of fatal car crashes in New York City during 2016 
[Instruction]: 
Your task is using data on the locations and related information of fatal car crashes in New York City 
to analyze fatal car crash distribution in 2016. You will use the car crash data along with the GeoJSON 
data of NYC administrative regions to plot the geographical distribution of fatal crashes in 2016 
on a map by geoplot. Save the figure as 'pred_results/collisions_map_vis.png'. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
17,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the geographical distribution of fatal car crashes in New York City during 2016 
[Instruction]: 
Your task is using data on the locations and related information of fatal car crashes in New York City 
to analyze fatal car crash distribution in 2016. You will use the car crash data along with the GeoJSON 
data of NYC administrative regions to plot the geographical distribution of fatal crashes in 2016 
on a map by geoplot. Save the figure as 'pred_results/collisions_map_vis.png'. 
[Domain Knowledge]: 
geoplot is a high-level Python geospatial plotting library. It is an extension to cartopy and matplotlib. 
The geoplot.pointplot() is a geospatial scatter plot function that plots each observation in your 
dataset as a single point on a map. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
17,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the geographical distribution of fatal car crashes in New York City during 2016 
[Instruction]: 
Your task is using data on the locations and related information of fatal car crashes in New York City 
to analyze fatal car crash distribution in 2016. You will use the car crash data along with the GeoJSON 
data of NYC administrative regions to plot the geographical distribution of fatal crashes in 2016 
on a map by geoplot. Save the figure as 'pred_results/collisions_map_vis.png'. 
[Dataset Description]: 
dataset/nyc_boroughs.geojson:Multipolygon Geojson file storing boroughs in the NewYork city. 

Columns of dataset/nyc_boroughs.geojson: 
'BoroCode', 'BoroName', 'Shape_Leng', 'Shape_Area', 'geometry' 

dataset/fatal_collisions.geojson: Point Geojson file storing fatal collisions in each boroughs 
along with the number of persons killed in 2016. 
Columns of dataset/fatal_collisions.geojson: 
'id', 'NUMBER OF PERSONS KILLED', 'BOROUGH', 'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
17,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the geographical distribution of fatal car crashes in New York City during 2016 
[Instruction]: 
Your task is using data on the locations and related information of fatal car crashes in New York City 
to analyze fatal car crash distribution in 2016. You will use the car crash data along with the GeoJSON 
data of NYC administrative regions to plot the geographical distribution of fatal crashes in 2016 
on a map by geoplot. Save the figure as 'pred_results/collisions_map_vis.png'. 
[Domain Knowledge]: 
geoplot is a high-level Python geospatial plotting library. It is an extension to cartopy and matplotlib. 
The geoplot.pointplot() is a geospatial scatter plot function that plots each observation in your 
dataset as a single point on a map. 
[Dataset Description]: 
dataset/nyc_boroughs.geojson:Multipolygon Geojson file storing boroughs in the NewYork city. 

Columns of dataset/nyc_boroughs.geojson: 
'BoroCode', 'BoroName', 'Shape_Leng', 'Shape_Area', 'geometry' 

dataset/fatal_collisions.geojson: Point Geojson file storing fatal collisions in each boroughs 
along with the number of persons killed in 2016. 
Columns of dataset/fatal_collisions.geojson: 
'id', 'NUMBER OF PERSONS KILLED', 'BOROUGH', 'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
18,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the street tree species data in San Francisco 
[Instruction]: 
Your task is using the distribution data data about San Francisco street trees and shapefiles of 
administrative regions to analyze the tree species NULL percentage in different regions. Visualize 
the results in a quadtree format map. Save the figure as 'pred_results/trees_count_vis.png'. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
18,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the street tree species data in San Francisco 
[Instruction]: 
Your task is using the distribution data data about San Francisco street trees and shapefiles of 
administrative regions to analyze the tree species NULL percentage in different regions. Visualize 
the results in a quadtree format map. Save the figure as 'pred_results/trees_count_vis.png'. 

[Domain Knowledge]: 
geoplot.quadtree() plots a choropleth map with point aggregate by quadtree neighborhoods. geodataframe.assign() 
function assigns new columns to a GeoDataFrame. A new column could be the data ""nullity"" percentage. 
geoplot.polyplot() plots a polygon data layer. The DataFrame.notnull() function in Pandas is 
used to detect non-missing values within a DataFrame. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
18,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the street tree species data in San Francisco 
[Instruction]: 
Your task is using the distribution data data about San Francisco street trees and shapefiles of 
administrative regions to analyze the tree species NULL percentage in different regions. Visualize 
the results in a quadtree format map. Save the figure as 'pred_results/trees_count_vis.png'. 

[Dataset Description]: 
dataset/street_trees_sample.geojson: Point Geojson file storing tree sample locations along 
with their species information. 
Columns of dataset/street_trees_sample.geojson 
'LegalStatus', 'Species', 'Address', 'SiteOrder', 'SiteInfo', 
'PlantType', 'Caretaker', 'CareAssistant', 'PlantDate', 'DBH', 
'PlotSize', 'PermitNotes', 'geometry' 

dataset/sfo_boroughs.geojson: One row of polygon Geojson file storing boroughs in the San Francisco. 

Columns of dataset/sfo_boroughs.geojson: 
'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
18,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the street tree species data in San Francisco 
[Instruction]: 
Your task is using the distribution data data about San Francisco street trees and shapefiles of 
administrative regions to analyze the tree species NULL percentage in different regions. Visualize 
the results in a quadtree format map. Save the figure as 'pred_results/trees_count_vis.png'. 

[Domain Knowledge]: 
geoplot.quadtree() plots a choropleth map with point aggregate by quadtree neighborhoods. geodataframe.assign() 
function assigns new columns to a GeoDataFrame. A new column could be the data ""nullity"" percentage. 
geoplot.polyplot() plots a polygon data layer. The DataFrame.notnull() function in Pandas is 
used to detect non-missing values within a DataFrame. 
[Dataset Description]: 
dataset/street_trees_sample.geojson: Point Geojson file storing tree sample locations along 
with their species information. 
Columns of dataset/street_trees_sample.geojson 
'LegalStatus', 'Species', 'Address', 'SiteOrder', 'SiteInfo', 
'PlantType', 'Caretaker', 'CareAssistant', 'PlantDate', 'DBH', 
'PlotSize', 'PermitNotes', 'geometry' 

dataset/sfo_boroughs.geojson: One row of polygon Geojson file storing boroughs in the San Francisco. 

Columns of dataset/sfo_boroughs.geojson: 
'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
19,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Modeling spatial patterns of water quality 
[Instruction]: 
Your task is model water quality using spatial interpolation techniques in Python. The analysis 
should focus on understanding spatial patterns of water quality by using point sample data and interpolating 
these values across a broader area including unsampled locations. Your goal is to load the water 
quality sample data, and apply appropriate spatial interpolation methods to predict water quality 
across the region. The final output should be a map showing the interpolated water quality surface, 
saved as ""pred_results/interploated_water_quality.png"" 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
19,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Modeling spatial patterns of water quality 
[Instruction]: 
Your task is model water quality using spatial interpolation techniques in Python. The analysis 
should focus on understanding spatial patterns of water quality by using point sample data and interpolating 
these values across a broader area including unsampled locations. Your goal is to load the water 
quality sample data, and apply appropriate spatial interpolation methods to predict water quality 
across the region. The final output should be a map showing the interpolated water quality surface, 
saved as ""pred_results/interploated_water_quality.png"" 
[Domain Knowledge]: 
Kernel Density Estimation (KDE) is a statistical method used in spatial analysis to estimate the 
probability density of a dataset. Unlike Kriging, KDE is non-parametric and does not rely on variogram 
modeling or assumptions about autocorrelation. It creates a smooth surface by applying a kernel 
function to each data point and summing their contributions over a spatial grid, making it ideal 
for interpolating values like water quality or species distribution without requiring strict 
assumptions about the data. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
19,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Modeling spatial patterns of water quality 
[Instruction]: 
Your task is model water quality using spatial interpolation techniques in Python. The analysis 
should focus on understanding spatial patterns of water quality by using point sample data and interpolating 
these values across a broader area including unsampled locations. Your goal is to load the water 
quality sample data, and apply appropriate spatial interpolation methods to predict water quality 
across the region. The final output should be a map showing the interpolated water quality surface, 
saved as ""pred_results/interploated_water_quality.png"" 
[Dataset Description]: 
dataset/DissolvedO2.geojson: Point Geojson file storing sampled O2 data in different area of 
the given bay area. 
Columns of dataset/DissolvedO2.geojson: 
'OBJECTID', 'HUC8', 'EventId', 'Cruise', 'Program', 'Project', 'Agency', 
'Source', 'Station', 'SampleDate', 'TotalDepth', 'UpperPycnocline', 
'LowerPycnocline', 'Depth', 'Layer', 'SampleType', 
'SampleReplicateType', 'Parameter', 'Qualifier', 'MeasureValue', 'Unit', 
'Method', 'Lab', 'Problem', 'PrecisionPC', 'BiasPC', 'Details', 
'Latitude', 'Longitude', 'geometry' 

dataset/Bay.geojson: One row of Multipolygon Geojson file storing the shape of the Bay area. 
Columns of dataset/Bay.geojson: 
'OBJECTID', 'Shape_Length', 'Shape_Area', 'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
19,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Modeling spatial patterns of water quality 
[Instruction]: 
Your task is model water quality using spatial interpolation techniques in Python. The analysis 
should focus on understanding spatial patterns of water quality by using point sample data and interpolating 
these values across a broader area including unsampled locations. Your goal is to load the water 
quality sample data, and apply appropriate spatial interpolation methods to predict water quality 
across the region. The final output should be a map showing the interpolated water quality surface, 
saved as ""pred_results/interploated_water_quality.png"" 
[Domain Knowledge]: 
Kernel Density Estimation (KDE) is a statistical method used in spatial analysis to estimate the 
probability density of a dataset. Unlike Kriging, KDE is non-parametric and does not rely on variogram 
modeling or assumptions about autocorrelation. It creates a smooth surface by applying a kernel 
function to each data point and summing their contributions over a spatial grid, making it ideal 
for interpolating values like water quality or species distribution without requiring strict 
assumptions about the data. 
[Dataset Description]: 
dataset/DissolvedO2.geojson: Point Geojson file storing sampled O2 data in different area of 
the given bay area. 
Columns of dataset/DissolvedO2.geojson: 
'OBJECTID', 'HUC8', 'EventId', 'Cruise', 'Program', 'Project', 'Agency', 
'Source', 'Station', 'SampleDate', 'TotalDepth', 'UpperPycnocline', 
'LowerPycnocline', 'Depth', 'Layer', 'SampleType', 
'SampleReplicateType', 'Parameter', 'Qualifier', 'MeasureValue', 'Unit', 
'Method', 'Lab', 'Problem', 'PrecisionPC', 'BiasPC', 'Details', 
'Latitude', 'Longitude', 'geometry' 

dataset/Bay.geojson: One row of Multipolygon Geojson file storing the shape of the Bay area. 
Columns of dataset/Bay.geojson: 
'OBJECTID', 'Shape_Length', 'Shape_Area', 'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
20,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Predict the likelihood of tin-tungsten deposits in Tasmania 
[Instruction]: 
Your task is performing a random forest prospectivity analysis for tin-tungsten deposits in Tasmania. 
The analysis should focus on training and evaluating a model using Python tools on some mineral ocurrence 
point data and geo-related evidence raster layers. Your goal is to set up the environment, load and 
inspect the data, build a random forest classifier model to identify the probability proximal of 
the area. The final auc should be at least 0.9. Save the final predicted graph as ""pred_results/mineral_prospectivity.png"" 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
20,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Predict the likelihood of tin-tungsten deposits in Tasmania 
[Instruction]: 
Your task is performing a random forest prospectivity analysis for tin-tungsten deposits in Tasmania. 
The analysis should focus on training and evaluating a model using Python tools on some mineral ocurrence 
point data and geo-related evidence raster layers. Your goal is to set up the environment, load and 
inspect the data, build a random forest classifier model to identify the probability proximal of 
the area. The final auc should be at least 0.9. Save the final predicted graph as ""pred_results/mineral_prospectivity.png"" 

[Domain Knowledge]: 
sklearn.ensemble.RandomForestClassifier() is a function to build a random forest classifier. 
To mitigate data imbalance,random undersampling randomly removes features from the major classes. 
""Buffer"" creates buffer polygons or zones around input geometry features to a specified distance. 
tasgrav_IR_1VD.tif tasmag_TMI.tif tasrad_Th_ppm.tif tasmag_TMI_1VD.tif tasrad_U_ppm.tif 
tasrad_K_pct.tif's data structure is similar to tasgrav_IR.tif. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
20,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Predict the likelihood of tin-tungsten deposits in Tasmania 
[Instruction]: 
Your task is performing a random forest prospectivity analysis for tin-tungsten deposits in Tasmania. 
The analysis should focus on training and evaluating a model using Python tools on some mineral ocurrence 
point data and geo-related evidence raster layers. Your goal is to set up the environment, load and 
inspect the data, build a random forest classifier model to identify the probability proximal of 
the area. The final auc should be at least 0.9. Save the final predicted graph as ""pred_results/mineral_prospectivity.png"" 

[Dataset Description]: 
dataset/sn_w_minoccs.gpkg: gpkg file storing point data of mineral ocurrence, Similar to point 
Geojson file. 
Columns of dataset/sn_w_minoccs.gpkg: 
'GID', 'DEPOSIT_ID', 'NAME', 'ALIAS', 'PARENT_ID', 'TYPE', 'COMM_TYPE', 
'COMMODITYS', 'DESCRIPT', 'LOCALITY', 'LOC_ACC', 'LOC_METHOD', 'RL', 
'RL_ACC', 'STATUS', 'STATUSDATE', 'DEP_SIZE', 'RESOURCES', 'FORM', 
'STRIKE', 'DIP', 'LITHOSTRAT', 'HOST_AGE', 'LITHOLOGY', 'ORE_DESCRT', 
'ORE_GENESS', 'MINERALAGE', 'GANGUE', 'EXPL_TYPE', 'OUTCROPS', 
'WEATHERED', 'XTRCTBLTY', 'USES', 'TESTS', 'GENETIC', 'DATA_MTLGA', 
'REF', 'geometry' 

Several GeoTIFF files with geo-related evidence raster layers: 
datasets\tasgrav_IR_1VD.tif 
datasets\tasgrav_IR.tif 
datasets\tasmag_TMI_1VD.tif 
datasets\tasmag_TMI.tif 
datasets\tasrad_K_pct.tif 
datasets\tasrad_Th_ppm.tif 
datasets\tasrad_U_ppm.tif 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
20,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Predict the likelihood of tin-tungsten deposits in Tasmania 
[Instruction]: 
Your task is performing a random forest prospectivity analysis for tin-tungsten deposits in Tasmania. 
The analysis should focus on training and evaluating a model using Python tools on some mineral ocurrence 
point data and geo-related evidence raster layers. Your goal is to set up the environment, load and 
inspect the data, build a random forest classifier model to identify the probability proximal of 
the area. The final auc should be at least 0.9. Save the final predicted graph as ""pred_results/mineral_prospectivity.png"" 

[Domain Knowledge]: 
sklearn.ensemble.RandomForestClassifier() is a function to build a random forest classifier. 
To mitigate data imbalance,random undersampling randomly removes features from the major classes. 
""Buffer"" creates buffer polygons or zones around input geometry features to a specified distance. 
tasgrav_IR_1VD.tif tasmag_TMI.tif tasrad_Th_ppm.tif tasmag_TMI_1VD.tif tasrad_U_ppm.tif 
tasrad_K_pct.tif's data structure is similar to tasgrav_IR.tif. 
[Dataset Description]: 
dataset/sn_w_minoccs.gpkg: gpkg file storing point data of mineral ocurrence, Similar to point 
Geojson file. 
Columns of dataset/sn_w_minoccs.gpkg: 
'GID', 'DEPOSIT_ID', 'NAME', 'ALIAS', 'PARENT_ID', 'TYPE', 'COMM_TYPE', 
'COMMODITYS', 'DESCRIPT', 'LOCALITY', 'LOC_ACC', 'LOC_METHOD', 'RL', 
'RL_ACC', 'STATUS', 'STATUSDATE', 'DEP_SIZE', 'RESOURCES', 'FORM', 
'STRIKE', 'DIP', 'LITHOSTRAT', 'HOST_AGE', 'LITHOLOGY', 'ORE_DESCRT', 
'ORE_GENESS', 'MINERALAGE', 'GANGUE', 'EXPL_TYPE', 'OUTCROPS', 
'WEATHERED', 'XTRCTBLTY', 'USES', 'TESTS', 'GENETIC', 'DATA_MTLGA', 
'REF', 'geometry' 

Several GeoTIFF files with geo-related evidence raster layers: 
datasets\tasgrav_IR_1VD.tif 
datasets\tasgrav_IR.tif 
datasets\tasmag_TMI_1VD.tif 
datasets\tasmag_TMI.tif 
datasets\tasrad_K_pct.tif 
datasets\tasrad_Th_ppm.tif 
datasets\tasrad_U_ppm.tif 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
21,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
You task is standardizing land cover and protected status datasets for consistency in analysis 
for mountain lion habitat. Reclassify these categorical datasets to a common scale using geospatial 
tools. Save the reclassified data as pred_results/landCover_reclassified.tif and pred_results/protected_status_reclassified.tif. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
21,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
You task is standardizing land cover and protected status datasets for consistency in analysis 
for mountain lion habitat. Reclassify these categorical datasets to a common scale using geospatial 
tools. Save the reclassified data as pred_results/landCover_reclassified.tif and pred_results/protected_status_reclassified.tif. 

[Domain Knowledge]: 
Reclassify is to change the input cell values in a raster in the purpose of standardlization or regrouping 
different categories. There are several approaches to reclassifying data, such as Individual 
values (Lookup, Reclassify), Ranges of values (Reclass by ASCII File, Reclass by Table, Reclassify), 
Intervals (Slice), and continuous values using functions (Rescale by Function). landCover_classification 
= { 11:10,#Open Water 21:8,#Developed, Open Space 22:7,#Developed, Low Intensity 23:8,#Developed, 
Medium Intensity 24:9,#Developed, High Intensity 31:6,#Barren Land 41:2,#Deciduous Forest 
42:1,#Evergreen Forest 43:2,#Mixed Forest 52:3,#Shrub/Scrub 71:3,#Grassland/Herbaceous 
72:3,#Sedge/Herbaceous 81:4,#Hay/Pasture 82:6,#Cultivated Crops 90:4,#Woody Wetlands 95:4,#Emergent 
Herbaceous Wetlands 255:10 } protected_status_classification = { 0:1, 1:3, 2:6, 3:9, 4:10, 255:10 
} 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
21,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
You task is standardizing land cover and protected status datasets for consistency in analysis 
for mountain lion habitat. Reclassify these categorical datasets to a common scale using geospatial 
tools. Save the reclassified data as pred_results/landCover_reclassified.tif and pred_results/protected_status_reclassified.tif. 

[Dataset Description]: 
dataset/landCover.tif: GeoTiff file storing the landcover types defined by National Land Cover 
Database 
Shape of dataset/landcover.tif: (2494, 3062) 

dataset/Protected_Status.tif: GeoTiff file storing the Protected status in 5 levels as well as 
the null data. 
Shape of dataset/Protected_Status.tif: (2494, 3062) 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
21,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
You task is standardizing land cover and protected status datasets for consistency in analysis 
for mountain lion habitat. Reclassify these categorical datasets to a common scale using geospatial 
tools. Save the reclassified data as pred_results/landCover_reclassified.tif and pred_results/protected_status_reclassified.tif. 

[Domain Knowledge]: 
Reclassify is to change the input cell values in a raster in the purpose of standardlization or regrouping 
different categories. There are several approaches to reclassifying data, such as Individual 
values (Lookup, Reclassify), Ranges of values (Reclass by ASCII File, Reclass by Table, Reclassify), 
Intervals (Slice), and continuous values using functions (Rescale by Function). landCover_classification 
= { 11:10,#Open Water 21:8,#Developed, Open Space 22:7,#Developed, Low Intensity 23:8,#Developed, 
Medium Intensity 24:9,#Developed, High Intensity 31:6,#Barren Land 41:2,#Deciduous Forest 
42:1,#Evergreen Forest 43:2,#Mixed Forest 52:3,#Shrub/Scrub 71:3,#Grassland/Herbaceous 
72:3,#Sedge/Herbaceous 81:4,#Hay/Pasture 82:6,#Cultivated Crops 90:4,#Woody Wetlands 95:4,#Emergent 
Herbaceous Wetlands 255:10 } protected_status_classification = { 0:1, 1:3, 2:6, 3:9, 4:10, 255:10 
} 
[Dataset Description]: 
dataset/landCover.tif: GeoTiff file storing the landcover types defined by National Land Cover 
Database 
Shape of dataset/landcover.tif: (2494, 3062) 

dataset/Protected_Status.tif: GeoTiff file storing the Protected status in 5 levels as well as 
the null data. 
Shape of dataset/Protected_Status.tif: (2494, 3062) 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
22,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is integrating ruggedness, road distance, land cover, and protected status into a single 
cost surface. Assign weights to each criterion to create a comprehensive habitat suitability map. 
Save the composite cost surface as ""pred_results/mountainLionCorridor.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
22,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is integrating ruggedness, road distance, land cover, and protected status into a single 
cost surface. Assign weights to each criterion to create a comprehensive habitat suitability map. 
Save the composite cost surface as ""pred_results/mountainLionCorridor.png"". 
[Domain Knowledge]: 
The Raster Calculator tool allows you to create and run map algebra expressions in a tool. The map 
algebra expressions allow users to enter mathematical (addition, division, and so on) and logical 
(greater than, equal to, and so on) operators in the expression to operate on raster layers. The general 
structure of a map algebra statement is an assignment operator (=), which is used to separate the 
action to its right from the name of the output (a raster object) to its left. The input raster layers 
need to have same size (col, row). Weight of each layer: distance: 1 ruggedness: 1.25 ProtectedStatus: 
1 landCover: 1.25 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
22,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is integrating ruggedness, road distance, land cover, and protected status into a single 
cost surface. Assign weights to each criterion to create a comprehensive habitat suitability map. 
Save the composite cost surface as ""pred_results/mountainLionCorridor.png"". 
[Dataset Description]: 
dataset/landCover_reclassified.tif: GeoTiff file storing the reclassified and normalized 
landcover types. 
Shape of dataset/landcover_reclassified.tif: (2494, 3062) 

dataset/Protected_Status_reclassified.tif: GeoTiff file storing the reclassified Protected 
status in 5 levels as well as the null data. 
Shape of dataset/Protected_Status_reclassified.tif: (2494, 3062) 

dataset/distance.tif: GeoTiff file storing the distance of each location to the nearest mountain 
lion habitats. 
Shape of dataset/distance.tif: (2494, 3062) 

dataset/ruggedness.tif: GeoTiff file storing the calculated ruggedness of the area. 
Shape of dataset/ruggedness.tif: (2494, 3062) 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
22,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is integrating ruggedness, road distance, land cover, and protected status into a single 
cost surface. Assign weights to each criterion to create a comprehensive habitat suitability map. 
Save the composite cost surface as ""pred_results/mountainLionCorridor.png"". 
[Domain Knowledge]: 
The Raster Calculator tool allows you to create and run map algebra expressions in a tool. The map 
algebra expressions allow users to enter mathematical (addition, division, and so on) and logical 
(greater than, equal to, and so on) operators in the expression to operate on raster layers. The general 
structure of a map algebra statement is an assignment operator (=), which is used to separate the 
action to its right from the name of the output (a raster object) to its left. The input raster layers 
need to have same size (col, row). Weight of each layer: distance: 1 ruggedness: 1.25 ProtectedStatus: 
1 landCover: 1.25 
[Dataset Description]: 
dataset/landCover_reclassified.tif: GeoTiff file storing the reclassified and normalized 
landcover types. 
Shape of dataset/landcover_reclassified.tif: (2494, 3062) 

dataset/Protected_Status_reclassified.tif: GeoTiff file storing the reclassified Protected 
status in 5 levels as well as the null data. 
Shape of dataset/Protected_Status_reclassified.tif: (2494, 3062) 

dataset/distance.tif: GeoTiff file storing the distance of each location to the nearest mountain 
lion habitats. 
Shape of dataset/distance.tif: (2494, 3062) 

dataset/ruggedness.tif: GeoTiff file storing the calculated ruggedness of the area. 
Shape of dataset/ruggedness.tif: (2494, 3062) 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
23,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Assess Open Space to Lower Flood Insurance Cost 
[Instruction]: 
Your task is assessing open space to lower flood insurance cost. The goal is to calculate the predominant 
land cover in each parcel and estimate open space area with accuracy of 85%. You should pay attention 
that small parcels contributed minimally can be excluded. You should integrate all information 
of OSP into a single table. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
23,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Assess Open Space to Lower Flood Insurance Cost 
[Instruction]: 
Your task is assessing open space to lower flood insurance cost. The goal is to calculate the predominant 
land cover in each parcel and estimate open space area with accuracy of 85%. You should pay attention 
that small parcels contributed minimally can be excluded. You should integrate all information 
of OSP into a single table. 
[Domain Knowledge]: 
Zonal Statistics as Table is summarizes the values of a raster within the zones of another dataset 
and reports the results as a table. In this example, the 2011 National Land Cover Database (NLCD) 
Impervious Surface Estimation layer has a measured accuracy of 85%. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
23,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Assess Open Space to Lower Flood Insurance Cost 
[Instruction]: 
Your task is assessing open space to lower flood insurance cost. The goal is to calculate the predominant 
land cover in each parcel and estimate open space area with accuracy of 85%. You should pay attention 
that small parcels contributed minimally can be excluded. You should integrate all information 
of OSP into a single table. 
[Dataset Description]: 
dataset/PADUS_CRS_final.tif: a raster file derived from the Protected Areas Database of the United 
States (PAD-US). It represents areas potentially eligible for open space preservation (OSP) within 
Georgetown County, South Carolina. 
dataset/GeorgetownCo_Parcels comprises vector data of property parcels within Georgetown County. 
Each parcel is delineated with its boundaries and associated attributes, such as parcel identification 
numbers (PARCEL_ID). 
dataset/PADUS_CRS_attrib.csv: serves as an attribute table for the PADUS_CRS_final.tif raster. 
It contains detailed information about each protected area, including attributes like OBJECTID, 
Category, and Ownership Type. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
23,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Assess Open Space to Lower Flood Insurance Cost 
[Instruction]: 
Your task is assessing open space to lower flood insurance cost. The goal is to calculate the predominant 
land cover in each parcel and estimate open space area with accuracy of 85%. You should pay attention 
that small parcels contributed minimally can be excluded. You should integrate all information 
of OSP into a single table. 
[Domain Knowledge]: 
Zonal Statistics as Table is summarizes the values of a raster within the zones of another dataset 
and reports the results as a table. In this example, the 2011 National Land Cover Database (NLCD) 
Impervious Surface Estimation layer has a measured accuracy of 85%. 
[Dataset Description]: 
dataset/PADUS_CRS_final.tif: a raster file derived from the Protected Areas Database of the United 
States (PAD-US). It represents areas potentially eligible for open space preservation (OSP) within 
Georgetown County, South Carolina. 
dataset/GeorgetownCo_Parcels comprises vector data of property parcels within Georgetown County. 
Each parcel is delineated with its boundaries and associated attributes, such as parcel identification 
numbers (PARCEL_ID). 
dataset/PADUS_CRS_attrib.csv: serves as an attribute table for the PADUS_CRS_final.tif raster. 
It contains detailed information about each protected area, including attributes like OBJECTID, 
Category, and Ownership Type. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
24,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Providing with a de-identified point-level dataset that includes all the variables of interest 
for each child, as well as their general location. 
[Instruction]: 
Your task is providing with a de-identified point-level dataset that includes all the variables 
of interest for each child, as well as their general location. You should load the point data and ensure 
Consistent Coordinate Reference System (CRS). You need to round the coordinates to two decimals 
and create new points features based on the added fields. The goal is to calculate and document the 
reduction in unique locations, Save the updated shapefile as a result. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
24,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Providing with a de-identified point-level dataset that includes all the variables of interest 
for each child, as well as their general location. 
[Instruction]: 
Your task is providing with a de-identified point-level dataset that includes all the variables 
of interest for each child, as well as their general location. You should load the point data and ensure 
Consistent Coordinate Reference System (CRS). You need to round the coordinates to two decimals 
and create new points features based on the added fields. The goal is to calculate and document the 
reduction in unique locations, Save the updated shapefile as a result. 
[Domain Knowledge]: 
Coordinate Reference Systems (CRS): Understanding the importance of using a consistent CRS (WGS 
1984 in this case) for accurate spatial analysis and comparison. Spatial De-identification: Rounding 
coordinates is a common method for spatial de-identification to protect individual privacy while 
maintaining general geographic patterns for analysis. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
24,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Providing with a de-identified point-level dataset that includes all the variables of interest 
for each child, as well as their general location. 
[Instruction]: 
Your task is providing with a de-identified point-level dataset that includes all the variables 
of interest for each child, as well as their general location. You should load the point data and ensure 
Consistent Coordinate Reference System (CRS). You need to round the coordinates to two decimals 
and create new points features based on the added fields. The goal is to calculate and document the 
reduction in unique locations, Save the updated shapefile as a result. 
[Dataset Description]: 
High_Blood_Level_Results.shp: This shapefile contains point data representing incidents of 
elevated blood lead levels in children within the study area. Each point corresponds to a specific 
geographic location where a blood test was conducted, and the results indicated a high concentration 
of lead. 

Columns of High_Blood_Level_Results.shp: 
'Address: House', 'Address: Street Name', 'Address: Unit', 'Address: City', 'Address: ZIP', 
'First Name', 'Last Name', 'Bday', 'Address: State', 'Blood Level', 'Blood Level Test Year', 'Race', 
'Gender', 'Age' 

Sacramento_ZIP_Codes.shp: This shapefile contains polygon data representing the boundaries 
of ZIP codes within Sacramento. Each polygon defines the geographic extent of a specific ZIP code. 


Columns of Sacramento_ZIP_Codes.shp: 
'Zip Code', 'Post Office Name', 'State Name', '2021 Total Population', 'Area in square miles', 
'Shape_Length', 'Shape_Area' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
24,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Providing with a de-identified point-level dataset that includes all the variables of interest 
for each child, as well as their general location. 
[Instruction]: 
Your task is providing with a de-identified point-level dataset that includes all the variables 
of interest for each child, as well as their general location. You should load the point data and ensure 
Consistent Coordinate Reference System (CRS). You need to round the coordinates to two decimals 
and create new points features based on the added fields. The goal is to calculate and document the 
reduction in unique locations, Save the updated shapefile as a result. 
[Domain Knowledge]: 
Coordinate Reference Systems (CRS): Understanding the importance of using a consistent CRS (WGS 
1984 in this case) for accurate spatial analysis and comparison. Spatial De-identification: Rounding 
coordinates is a common method for spatial de-identification to protect individual privacy while 
maintaining general geographic patterns for analysis. 
[Dataset Description]: 
High_Blood_Level_Results.shp: This shapefile contains point data representing incidents of 
elevated blood lead levels in children within the study area. Each point corresponds to a specific 
geographic location where a blood test was conducted, and the results indicated a high concentration 
of lead. 

Columns of High_Blood_Level_Results.shp: 
'Address: House', 'Address: Street Name', 'Address: Unit', 'Address: City', 'Address: ZIP', 
'First Name', 'Last Name', 'Bday', 'Address: State', 'Blood Level', 'Blood Level Test Year', 'Race', 
'Gender', 'Age' 

Sacramento_ZIP_Codes.shp: This shapefile contains polygon data representing the boundaries 
of ZIP codes within Sacramento. Each polygon defines the geographic extent of a specific ZIP code. 


Columns of Sacramento_ZIP_Codes.shp: 
'Zip Code', 'Post Office Name', 'State Name', '2021 Total Population', 'Area in square miles', 
'Shape_Length', 'Shape_Area' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
25,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Create risk maps for transmission, susceptibility, and resource scarcity. Then create a map of 
risk profiles to help pinpoint targeted intervention areas. 
[Instruction]: 
Your task is to analyze COVID-19 risk in HKG using Arcpy. First, use similarity search to map transmission 
risk from hkg_constituency.shp and target_risk.shp. Second, use add join, calculate field and 
remove join to join transmission risk map to the hkg_constituency.shp. Then do the same search and 
join functions for the other three risk map (susceptibility risk, healthcare resource scarcity 
risk, and exposure risk). Finally, use multivariate clustering to map the COVID-19 risk profiles 
of HKG. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
25,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Create risk maps for transmission, susceptibility, and resource scarcity. Then create a map of 
risk profiles to help pinpoint targeted intervention areas. 
[Instruction]: 
Your task is to analyze COVID-19 risk in HKG using Arcpy. First, use similarity search to map transmission 
risk from hkg_constituency.shp and target_risk.shp. Second, use add join, calculate field and 
remove join to join transmission risk map to the hkg_constituency.shp. Then do the same search and 
join functions for the other three risk map (susceptibility risk, healthcare resource scarcity 
risk, and exposure risk). Finally, use multivariate clustering to map the COVID-19 risk profiles 
of HKG. 
[Domain Knowledge]: 
For Search Similarity: Number Of Results = 0 Fields To Append To Output = ""ID"" Attributes Of Interest: 
""PopDensity;SpInterIdx"" for transmission risk. ""PopDensity;Seniors60t;spending_c;keyfacts_p"" 
for susceptibility risk. ""Seniors60t;SUM_Beds"" for insufficient resource risk. ""RelativeCa"" 
for exposure risk. For Join Functions: use ""ID"" as join field, and calculate the new field using the 
expression ""432 - !join_table.SIMRANK!"" For Multivariate Clustering: select the four target 
fields and use K means as clustering method, ""SEEDS"" as initialization fields. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
25,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Create risk maps for transmission, susceptibility, and resource scarcity. Then create a map of 
risk profiles to help pinpoint targeted intervention areas. 
[Instruction]: 
Your task is to analyze COVID-19 risk in HKG using Arcpy. First, use similarity search to map transmission 
risk from hkg_constituency.shp and target_risk.shp. Second, use add join, calculate field and 
remove join to join transmission risk map to the hkg_constituency.shp. Then do the same search and 
join functions for the other three risk map (susceptibility risk, healthcare resource scarcity 
risk, and exposure risk). Finally, use multivariate clustering to map the COVID-19 risk profiles 
of HKG. 
[Dataset Description]: 
dataset/hkg_constituency.shp: This shapefile contains data for 431 Hong Kong constituencies. 
The data includes demographic and spatial information that can be used to assess the risk of disease 
transmission, COVID- 19 susceptibility, and healthcare resource scarcity. 
Key Features: 
'ID', 'Constituen', 'TotalPop', 'PopDensity', 'Seniors60t', 'spending_c', 'keyfacts_p', 
'SUM_Beds', 'SpInterIdx', 'RelativeCa', 'SEEDS' 

dataset/target_risk.shp: The attributes for the target_risk shapefile are similar to those for 
the hkg_constituency.shp. The values in the target_risk, however, are the worst-case values found 
throughout Hong Kong. They include the densest population value, the largest number of seniors 
per 1,000 people, the highest index for tobacco, the smallest purchasing power value, and so on. 
These worst-case values will be used as the target against which all other constituencies will be 
ranked in order to determine risk. 
Key Features: 
'ID', 'Constituen', 'TotalPop', 'PopDensity', 'Seniors60t', 'spending_c', 'keyfacts_p', 
'SUM_Beds', 'SpInterIdx', 'RelativeCa' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
25,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Create risk maps for transmission, susceptibility, and resource scarcity. Then create a map of 
risk profiles to help pinpoint targeted intervention areas. 
[Instruction]: 
Your task is to analyze COVID-19 risk in HKG using Arcpy. First, use similarity search to map transmission 
risk from hkg_constituency.shp and target_risk.shp. Second, use add join, calculate field and 
remove join to join transmission risk map to the hkg_constituency.shp. Then do the same search and 
join functions for the other three risk map (susceptibility risk, healthcare resource scarcity 
risk, and exposure risk). Finally, use multivariate clustering to map the COVID-19 risk profiles 
of HKG. 
[Domain Knowledge]: 
For Search Similarity: Number Of Results = 0 Fields To Append To Output = ""ID"" Attributes Of Interest: 
""PopDensity;SpInterIdx"" for transmission risk. ""PopDensity;Seniors60t;spending_c;keyfacts_p"" 
for susceptibility risk. ""Seniors60t;SUM_Beds"" for insufficient resource risk. ""RelativeCa"" 
for exposure risk. For Join Functions: use ""ID"" as join field, and calculate the new field using the 
expression ""432 - !join_table.SIMRANK!"" For Multivariate Clustering: select the four target 
fields and use K means as clustering method, ""SEEDS"" as initialization fields. 
[Dataset Description]: 
dataset/hkg_constituency.shp: This shapefile contains data for 431 Hong Kong constituencies. 
The data includes demographic and spatial information that can be used to assess the risk of disease 
transmission, COVID- 19 susceptibility, and healthcare resource scarcity. 
Key Features: 
'ID', 'Constituen', 'TotalPop', 'PopDensity', 'Seniors60t', 'spending_c', 'keyfacts_p', 
'SUM_Beds', 'SpInterIdx', 'RelativeCa', 'SEEDS' 

dataset/target_risk.shp: The attributes for the target_risk shapefile are similar to those for 
the hkg_constituency.shp. The values in the target_risk, however, are the worst-case values found 
throughout Hong Kong. They include the densest population value, the largest number of seniors 
per 1,000 people, the highest index for tobacco, the smallest purchasing power value, and so on. 
These worst-case values will be used as the target against which all other constituencies will be 
ranked in order to determine risk. 
Key Features: 
'ID', 'Constituen', 'TotalPop', 'PopDensity', 'Seniors60t', 'spending_c', 'keyfacts_p', 
'SUM_Beds', 'SpInterIdx', 'RelativeCa' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
26,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Use drainage conditions and water depth to calculate groundwater vulnerable areas. 
[Instruction]: 
Your task is to calculate groundwater vulnerable areas using the Arcpy. First, project 'mc_land_cover.tif' 
and 'mc_soils.shp' to Lambert Conformal Conic. Next, using 'mc_boundary.shp' as the extent, land_cover 
as the cellSize and snapRaster, extract the drainage_conditions and water_depth respectively 
from 'mc_soils.shp'. Then, use drainage_conditions and water_depth to perform suitability modeling 
to calculate vulnerable_areas. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
26,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Use drainage conditions and water depth to calculate groundwater vulnerable areas. 
[Instruction]: 
Your task is to calculate groundwater vulnerable areas using the Arcpy. First, project 'mc_land_cover.tif' 
and 'mc_soils.shp' to Lambert Conformal Conic. Next, using 'mc_boundary.shp' as the extent, land_cover 
as the cellSize and snapRaster, extract the drainage_conditions and water_depth respectively 
from 'mc_soils.shp'. Then, use drainage_conditions and water_depth to perform suitability modeling 
to calculate vulnerable_areas. 
[Domain Knowledge]: 
Suitability modeling is an analytical process used to identify the optimal location or suitability 
of geographic areas. It generally consists of three steps: data preparation, data reclassification, 
and weighted overlay. In ArcGIS Pro, suitability modeling is implemented through an integrated 
Modeler, but in Arcpy, it needs to be carried out step by step using basic functions. For preparation: 
Polygon data needs to be converted into Raster before suitability modeling. For reclassification: 
The reclassification of drainage_conditions is [[1, 3], [2, 1], [3, 4], [4, 5], [5, 2]]. The reclassification 
of water_depth is [[0, 10, 5], [10, 33, 4], [33, 61, 3], [61, 80, 2], [80, 92, 1]]. For weighted overlay: 
vulnerable_areas = drainage_conditions * 5 + water_depth * 4 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
26,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Use drainage conditions and water depth to calculate groundwater vulnerable areas. 
[Instruction]: 
Your task is to calculate groundwater vulnerable areas using the Arcpy. First, project 'mc_land_cover.tif' 
and 'mc_soils.shp' to Lambert Conformal Conic. Next, using 'mc_boundary.shp' as the extent, land_cover 
as the cellSize and snapRaster, extract the drainage_conditions and water_depth respectively 
from 'mc_soils.shp'. Then, use drainage_conditions and water_depth to perform suitability modeling 
to calculate vulnerable_areas. 
[Dataset Description]: 
dataset/mc_soils.shp: In this shapefile, three fields are important in groundwater analysis: 
Drainage Class – Dominant Conditions, Hydrologic Group – Dominant Conditions, and Water Table 
Depth – Annual – Minimum. 
Key Fields: 
'FID', 'Shape', 'AREASYMBOL', 'SPATIALVER', 'MUSYM', 'MUKEY', 'drclassdcd', 'hydgrpdcd', 
'wdepannmin' 

dataset/mc_boundary.shp: This shapefile contains the polygon region of study area. 
Key Fields: 
'FID', 'Shape', 'OBJECTID_1', 'AREA', 'PERIMETER', 'BLMCNTYO_', 'BLMCNTYO_I', 'COUNTY_NAM', 
'COBCODE' 

dataset/mc_land_cover.tif: Raster file showing the land cover of study area, one band. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
26,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Use drainage conditions and water depth to calculate groundwater vulnerable areas. 
[Instruction]: 
Your task is to calculate groundwater vulnerable areas using the Arcpy. First, project 'mc_land_cover.tif' 
and 'mc_soils.shp' to Lambert Conformal Conic. Next, using 'mc_boundary.shp' as the extent, land_cover 
as the cellSize and snapRaster, extract the drainage_conditions and water_depth respectively 
from 'mc_soils.shp'. Then, use drainage_conditions and water_depth to perform suitability modeling 
to calculate vulnerable_areas. 
[Domain Knowledge]: 
Suitability modeling is an analytical process used to identify the optimal location or suitability 
of geographic areas. It generally consists of three steps: data preparation, data reclassification, 
and weighted overlay. In ArcGIS Pro, suitability modeling is implemented through an integrated 
Modeler, but in Arcpy, it needs to be carried out step by step using basic functions. For preparation: 
Polygon data needs to be converted into Raster before suitability modeling. For reclassification: 
The reclassification of drainage_conditions is [[1, 3], [2, 1], [3, 4], [4, 5], [5, 2]]. The reclassification 
of water_depth is [[0, 10, 5], [10, 33, 4], [33, 61, 3], [61, 80, 2], [80, 92, 1]]. For weighted overlay: 
vulnerable_areas = drainage_conditions * 5 + water_depth * 4 
[Dataset Description]: 
dataset/mc_soils.shp: In this shapefile, three fields are important in groundwater analysis: 
Drainage Class – Dominant Conditions, Hydrologic Group – Dominant Conditions, and Water Table 
Depth – Annual – Minimum. 
Key Fields: 
'FID', 'Shape', 'AREASYMBOL', 'SPATIALVER', 'MUSYM', 'MUKEY', 'drclassdcd', 'hydgrpdcd', 
'wdepannmin' 

dataset/mc_boundary.shp: This shapefile contains the polygon region of study area. 
Key Fields: 
'FID', 'Shape', 'OBJECTID_1', 'AREA', 'PERIMETER', 'BLMCNTYO_', 'BLMCNTYO_I', 'COUNTY_NAM', 
'COBCODE' 

dataset/mc_land_cover.tif: Raster file showing the land cover of study area, one band. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
27,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Identify undeveloped areas from groundwater risk zones. 
[Instruction]: 
Your task is to identify those high risk zones of groundwater vulnerable areas that need protection 
using the Arcpy. First, use vulnerable_areas and land_cover to perform suitability modeling to 
calculate risk_zones. Next, filter out high_risk_zones, and then undeveloped_areas based on 
risk_zones, and save the result to 'output/undeveloped_areas.tif'. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
27,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Identify undeveloped areas from groundwater risk zones. 
[Instruction]: 
Your task is to identify those high risk zones of groundwater vulnerable areas that need protection 
using the Arcpy. First, use vulnerable_areas and land_cover to perform suitability modeling to 
calculate risk_zones. Next, filter out high_risk_zones, and then undeveloped_areas based on 
risk_zones, and save the result to 'output/undeveloped_areas.tif'. 
[Domain Knowledge]: 
Suitability modeling is an analytical process used to identify the optimal location or suitability 
of geographic areas. It generally consists of three steps: data preparation, data reclassification, 
and weighted overlay. In ArcGIS Pro, suitability modeling is implemented through an integrated 
Modeler, but in Arcpy, it needs to be carried out step by step using basic functions. The reclassification 
of vulnerable_areas is standardization to [1, 10]. The reclassification of land_cover is [[11, 
1],[21, 6],[22, 7],[23, 8],[24, 10],[31, 4],[41, 3],[42, 1],[43, 3],[52, 3],[71, 2],[81, 5],[82, 
9],[90, 1],[95, 1]]. risk_zones = vulnerable_areas * 8 + land_cover * 10 Undeveloped areas in the 
high risk zones (risk_zones > 100) need more protection. Seek for the areas in land_cover masking 
high risk zone, use the following rule: where_clause=""Class IN ('Deciduous Forest', 'Emergent 
Herbaceous Wetlands', 'Hay/Pasture', 'Herbaceous', 'Mixed Forest', 'Shrub/Scrub', 'Woody 
Wetlands', 'Barren Land')"" 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
27,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Identify undeveloped areas from groundwater risk zones. 
[Instruction]: 
Your task is to identify those high risk zones of groundwater vulnerable areas that need protection 
using the Arcpy. First, use vulnerable_areas and land_cover to perform suitability modeling to 
calculate risk_zones. Next, filter out high_risk_zones, and then undeveloped_areas based on 
risk_zones, and save the result to 'output/undeveloped_areas.tif'. 
[Dataset Description]: 
dataset/vulnerable_areas.tif: Raster file showing the whole groundwater vulnerable areas of 
study area, one band. 

dataset/land_cover.tif: Raster file showing the land cover of study area, one band. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
27,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Identify undeveloped areas from groundwater risk zones. 
[Instruction]: 
Your task is to identify those high risk zones of groundwater vulnerable areas that need protection 
using the Arcpy. First, use vulnerable_areas and land_cover to perform suitability modeling to 
calculate risk_zones. Next, filter out high_risk_zones, and then undeveloped_areas based on 
risk_zones, and save the result to 'output/undeveloped_areas.tif'. 
[Domain Knowledge]: 
Suitability modeling is an analytical process used to identify the optimal location or suitability 
of geographic areas. It generally consists of three steps: data preparation, data reclassification, 
and weighted overlay. In ArcGIS Pro, suitability modeling is implemented through an integrated 
Modeler, but in Arcpy, it needs to be carried out step by step using basic functions. The reclassification 
of vulnerable_areas is standardization to [1, 10]. The reclassification of land_cover is [[11, 
1],[21, 6],[22, 7],[23, 8],[24, 10],[31, 4],[41, 3],[42, 1],[43, 3],[52, 3],[71, 2],[81, 5],[82, 
9],[90, 1],[95, 1]]. risk_zones = vulnerable_areas * 8 + land_cover * 10 Undeveloped areas in the 
high risk zones (risk_zones > 100) need more protection. Seek for the areas in land_cover masking 
high risk zone, use the following rule: where_clause=""Class IN ('Deciduous Forest', 'Emergent 
Herbaceous Wetlands', 'Hay/Pasture', 'Herbaceous', 'Mixed Forest', 'Shrub/Scrub', 'Woody 
Wetlands', 'Barren Land')"" 
[Dataset Description]: 
dataset/vulnerable_areas.tif: Raster file showing the whole groundwater vulnerable areas of 
study area, one band. 

dataset/land_cover.tif: Raster file showing the land cover of study area, one band. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
28,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Estimate the origin-destination (OD) flows between regions based on the socioeconomic attributes 
of regions and the mobility data. 
[Instruction]: 
Your task is to estimate the interaction strengths between subregions using a Random Forest model. 
First, load the OD flow data and socio-economic attribute data for each subregion. Second, aggregate 
the OD flow data to the subregion scale to create subregion spatial interaction data. Third, prepare 
the dataset by merging the aggregated OD flows with the socio-economic attributes of origin and 
destination subregions. Finally, build a Random Forest model with hyperparameter tuning using 
GridSearchCV to predict interaction strengths and evaluate the model performance using mean squared 
error. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
28,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Estimate the origin-destination (OD) flows between regions based on the socioeconomic attributes 
of regions and the mobility data. 
[Instruction]: 
Your task is to estimate the interaction strengths between subregions using a Random Forest model. 
First, load the OD flow data and socio-economic attribute data for each subregion. Second, aggregate 
the OD flow data to the subregion scale to create subregion spatial interaction data. Third, prepare 
the dataset by merging the aggregated OD flows with the socio-economic attributes of origin and 
destination subregions. Finally, build a Random Forest model with hyperparameter tuning using 
GridSearchCV to predict interaction strengths and evaluate the model performance using mean squared 
error. 
[Domain Knowledge]: 
OD Flows: Origin-Destination (OD) flows represent the movement volume between locations, crucial 
for understanding regional connectivity, transportation, and economic interactions. Random 
Forest (RF) Model: An ensemble machine learning method that builds multiple decision trees to improve 
prediction accuracy and feature importance analysis. RF is effective for modeling complex, non-linear 
relationships, making it suitable for analyzing factors influencing OD flows. GridSearchCV: 
GridSearchCV is a hyperparameter tuning tool that systematically tests combinations of specified 
hyperparameter values to find the best configuration for a machine learning model. It performs 
cross-validation for each parameter set to evaluate performance, ensuring robust and optimal 
model settings for improved accuracy and generalization. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
28,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Estimate the origin-destination (OD) flows between regions based on the socioeconomic attributes 
of regions and the mobility data. 
[Instruction]: 
Your task is to estimate the interaction strengths between subregions using a Random Forest model. 
First, load the OD flow data and socio-economic attribute data for each subregion. Second, aggregate 
the OD flow data to the subregion scale to create subregion spatial interaction data. Third, prepare 
the dataset by merging the aggregated OD flows with the socio-economic attributes of origin and 
destination subregions. Finally, build a Random Forest model with hyperparameter tuning using 
GridSearchCV to predict interaction strengths and evaluate the model performance using mean squared 
error. 
[Dataset Description]: 
dataset/od_data.csv: Represents the movement volume between subregions, used for analyzing 
interaction strengths and regional connectivity. 
Columns of dataset/od_data.csv include Origin, Destination, and FlowVolume. 

dataset/socioeconomic_data.csv: Contains socio-economic details for each subregion, providing 
contextual data that can be merged with OD flow data to enhance analysis by incorporating socio-economic 
factors influencing interactions. 
Columns of dataset/socioeconomic_data.csv include Subregion, Population, GDP, EmploymentRate, 
AverageIncome, EducationIndex, and HealthcareAccess. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
28,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Estimate the origin-destination (OD) flows between regions based on the socioeconomic attributes 
of regions and the mobility data. 
[Instruction]: 
Your task is to estimate the interaction strengths between subregions using a Random Forest model. 
First, load the OD flow data and socio-economic attribute data for each subregion. Second, aggregate 
the OD flow data to the subregion scale to create subregion spatial interaction data. Third, prepare 
the dataset by merging the aggregated OD flows with the socio-economic attributes of origin and 
destination subregions. Finally, build a Random Forest model with hyperparameter tuning using 
GridSearchCV to predict interaction strengths and evaluate the model performance using mean squared 
error. 
[Domain Knowledge]: 
OD Flows: Origin-Destination (OD) flows represent the movement volume between locations, crucial 
for understanding regional connectivity, transportation, and economic interactions. Random 
Forest (RF) Model: An ensemble machine learning method that builds multiple decision trees to improve 
prediction accuracy and feature importance analysis. RF is effective for modeling complex, non-linear 
relationships, making it suitable for analyzing factors influencing OD flows. GridSearchCV: 
GridSearchCV is a hyperparameter tuning tool that systematically tests combinations of specified 
hyperparameter values to find the best configuration for a machine learning model. It performs 
cross-validation for each parameter set to evaluate performance, ensuring robust and optimal 
model settings for improved accuracy and generalization. 
[Dataset Description]: 
dataset/od_data.csv: Represents the movement volume between subregions, used for analyzing 
interaction strengths and regional connectivity. 
Columns of dataset/od_data.csv include Origin, Destination, and FlowVolume. 

dataset/socioeconomic_data.csv: Contains socio-economic details for each subregion, providing 
contextual data that can be merged with OD flow data to enhance analysis by incorporating socio-economic 
factors influencing interactions. 
Columns of dataset/socioeconomic_data.csv include Subregion, Population, GDP, EmploymentRate, 
AverageIncome, EducationIndex, and HealthcareAccess. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
29,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate Travel Time for a Tsunami 
[Instruction]: 
To calculate tsunami travel time, set up your project with bathymetric and coastal data, then identify 
the tsunami origin. Simulate wave travel over the bathymetric surface to create an arrival time 
map. Visualize and analyze the results, adjust for accuracy, and prepare a report to aid in preparedness 
and response planning. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
29,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate Travel Time for a Tsunami 
[Instruction]: 
To calculate tsunami travel time, set up your project with bathymetric and coastal data, then identify 
the tsunami origin. Simulate wave travel over the bathymetric surface to create an arrival time 
map. Visualize and analyze the results, adjust for accuracy, and prepare a report to aid in preparedness 
and response planning. 
[Domain Knowledge]: 
To calculate tsunami speed using √(g*d) , where d is ocean depth, you'll need positive depth values. 
Since AtlanticDEM has negative values for ocean depths (below sea level), you'll modify the expression 
to invert these values, making ocean areas positive for accurate calculations. Geodetic Densify 
creates new features by replacing the segments of the input features with densified approximations 
of geodesic segments. Extract by Mask tool is used to extract cell values from a raster using the shape 
of a feature or another raster as a mask. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
29,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate Travel Time for a Tsunami 
[Instruction]: 
To calculate tsunami travel time, set up your project with bathymetric and coastal data, then identify 
the tsunami origin. Simulate wave travel over the bathymetric surface to create an arrival time 
map. Visualize and analyze the results, adjust for accuracy, and prepare a report to aid in preparedness 
and response planning. 
[Dataset Description]: 
dataset/AtlanticDEM.lyrx: A DEM layer representing Atlantic Ocean bathymetry, essential for 
analyzing how ocean depth affects tsunami wave propagation. Raster data 
dataset/Tsunami Paths.lyrx: 
A layer showing precomputed tsunami travel paths and travel times, used to visualize and analyze 
tsunami wave routes and their impact on coastal areas. Attributes: OBJECTID, Shape, lengthkm, 
Shape_Length, Route 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
29,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate Travel Time for a Tsunami 
[Instruction]: 
To calculate tsunami travel time, set up your project with bathymetric and coastal data, then identify 
the tsunami origin. Simulate wave travel over the bathymetric surface to create an arrival time 
map. Visualize and analyze the results, adjust for accuracy, and prepare a report to aid in preparedness 
and response planning. 
[Domain Knowledge]: 
To calculate tsunami speed using √(g*d) , where d is ocean depth, you'll need positive depth values. 
Since AtlanticDEM has negative values for ocean depths (below sea level), you'll modify the expression 
to invert these values, making ocean areas positive for accurate calculations. Geodetic Densify 
creates new features by replacing the segments of the input features with densified approximations 
of geodesic segments. Extract by Mask tool is used to extract cell values from a raster using the shape 
of a feature or another raster as a mask. 
[Dataset Description]: 
dataset/AtlanticDEM.lyrx: A DEM layer representing Atlantic Ocean bathymetry, essential for 
analyzing how ocean depth affects tsunami wave propagation. Raster data 
dataset/Tsunami Paths.lyrx: 
A layer showing precomputed tsunami travel paths and travel times, used to visualize and analyze 
tsunami wave routes and their impact on coastal areas. Attributes: OBJECTID, Shape, lengthkm, 
Shape_Length, Route 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
30,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Designate bike routes for commuting professionals 
[Instruction]: 
Your task is to design optimal bike routes for a commuter network. Analyze demand hotspots using 
spatial clustering tools.Evaluate existing infrastructure by overlaying bike lane and road network 
data. Apply suitability analysis by combining traffic volume, road slope, and safety data using 
a weighted overlay. Use Network Analyst to propose new routes, ensuring connectivity to major employment 
centers and evaluating the solution through accessibility metrics like coverage within a half-mile 
radius. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
30,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Designate bike routes for commuting professionals 
[Instruction]: 
Your task is to design optimal bike routes for a commuter network. Analyze demand hotspots using 
spatial clustering tools.Evaluate existing infrastructure by overlaying bike lane and road network 
data. Apply suitability analysis by combining traffic volume, road slope, and safety data using 
a weighted overlay. Use Network Analyst to propose new routes, ensuring connectivity to major employment 
centers and evaluating the solution through accessibility metrics like coverage within a half-mile 
radius. 
[Domain Knowledge]: 
The City of Seattle plans to construct protected bike lanes, prioritizing busier streets to identify 
direct routes. Using attribute selection, all arterial roads were filtered with codes 1 for segment 
type and 1 for arterial roads, creating a new layer with copied features and clearing selections 
in the street network attribute table. The design divides into three sections: East, where E Madison 
St in Capitol Hill and Madison St downtown were identified; West, where 10th Ave E offers excellent 
coverage across the western side; and Central, where 23rd Ave, 23rd Ave E, 24th Ave E, and Turner Way 
E align with E Madison St. Most of the block lies within a half-mile of these selected streets. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
30,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Designate bike routes for commuting professionals 
[Instruction]: 
Your task is to design optimal bike routes for a commuter network. Analyze demand hotspots using 
spatial clustering tools.Evaluate existing infrastructure by overlaying bike lane and road network 
data. Apply suitability analysis by combining traffic volume, road slope, and safety data using 
a weighted overlay. Use Network Analyst to propose new routes, ensuring connectivity to major employment 
centers and evaluating the solution through accessibility metrics like coverage within a half-mile 
radius. 
[Dataset Description]: 
dataset/Neighborhoods: Spatial polygons representing different urban and suburban areas, often 
linked with demographic or socioeconomic data. 
dataset/Streets: Line features depicting road networks, including attributes like road type, 
traffic volume, and bike lane presence. 
dataset/Zoning: Polygons indicating land use designations such as residential, commercial, 
industrial, or mixed-use, often tied to urban planning and development policies. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
30,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Designate bike routes for commuting professionals 
[Instruction]: 
Your task is to design optimal bike routes for a commuter network. Analyze demand hotspots using 
spatial clustering tools.Evaluate existing infrastructure by overlaying bike lane and road network 
data. Apply suitability analysis by combining traffic volume, road slope, and safety data using 
a weighted overlay. Use Network Analyst to propose new routes, ensuring connectivity to major employment 
centers and evaluating the solution through accessibility metrics like coverage within a half-mile 
radius. 
[Domain Knowledge]: 
The City of Seattle plans to construct protected bike lanes, prioritizing busier streets to identify 
direct routes. Using attribute selection, all arterial roads were filtered with codes 1 for segment 
type and 1 for arterial roads, creating a new layer with copied features and clearing selections 
in the street network attribute table. The design divides into three sections: East, where E Madison 
St in Capitol Hill and Madison St downtown were identified; West, where 10th Ave E offers excellent 
coverage across the western side; and Central, where 23rd Ave, 23rd Ave E, 24th Ave E, and Turner Way 
E align with E Madison St. Most of the block lies within a half-mile of these selected streets. 
[Dataset Description]: 
dataset/Neighborhoods: Spatial polygons representing different urban and suburban areas, often 
linked with demographic or socioeconomic data. 
dataset/Streets: Line features depicting road networks, including attributes like road type, 
traffic volume, and bike lane presence. 
dataset/Zoning: Polygons indicating land use designations such as residential, commercial, 
industrial, or mixed-use, often tied to urban planning and development policies. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
31,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Detect aggregation scales of geographical flows 
[Instruction]: 
Your task is to analyze the spatial aggregation patterns and scales of geographical flows using 
the L-function. First, load the flow data, origin area data , and destination area data. Second, 
calculate the local K-function for each flow by iterating over flow pairs and applying distance-based 
analysis with edge correction. Third, compute the global L-function by averaging the local K-functions 
and transforming the result. Finally, visualize the L-function with confidence intervals and 
critical distance markers for interpretation. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
31,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Detect aggregation scales of geographical flows 
[Instruction]: 
Your task is to analyze the spatial aggregation patterns and scales of geographical flows using 
the L-function. First, load the flow data, origin area data , and destination area data. Second, 
calculate the local K-function for each flow by iterating over flow pairs and applying distance-based 
analysis with edge correction. Third, compute the global L-function by averaging the local K-functions 
and transforming the result. Finally, visualize the L-function with confidence intervals and 
critical distance markers for interpretation. 
[Domain Knowledge]: 
Complete Spatial Randomness (CSR): a baseline model in spatial analysis, assuming a uniform, random 
distribution of flows across space. Deviations from CSR indicate clustering or dispersion, which 
the L-function quantifies. K-function: Measures the cumulative density of flows within a given 
distance, normalized by the total flow intensity. L-function: A transformed version of the K-function, 
making it easier to interpret deviations from CSR: L(r) > 0: Clustering, L(r)<0: Dispesion 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
31,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Detect aggregation scales of geographical flows 
[Instruction]: 
Your task is to analyze the spatial aggregation patterns and scales of geographical flows using 
the L-function. First, load the flow data, origin area data , and destination area data. Second, 
calculate the local K-function for each flow by iterating over flow pairs and applying distance-based 
analysis with edge correction. Third, compute the global L-function by averaging the local K-functions 
and transforming the result. Finally, visualize the L-function with confidence intervals and 
critical distance markers for interpretation. 
[Dataset Description]: 
dataset/flow.csv: Contains the geographical flows represented as connected origin-destination 
(OD) point pairs. 
Columns: x_o: X-coordinate of the origin point; y_o: Y-coordinate of the origin point; x_d: X-coordinate 
of the destination point; y_d: Y-coordinate of the destination point. 

dataset/o_area.csv: Contains the boundary coordinates for the region where the origin points 
lie. 
Columns: x: X-coordinate of the boundary point; y: Y-coordinate of the boundary point. 

dataset/d_area.csv: Contains the boundary coordinates for the region where the destination points 
lie. 
Columns: x: X-coordinate of the boundary point; y: Y-coordinate of the boundary point. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
31,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Detect aggregation scales of geographical flows 
[Instruction]: 
Your task is to analyze the spatial aggregation patterns and scales of geographical flows using 
the L-function. First, load the flow data, origin area data , and destination area data. Second, 
calculate the local K-function for each flow by iterating over flow pairs and applying distance-based 
analysis with edge correction. Third, compute the global L-function by averaging the local K-functions 
and transforming the result. Finally, visualize the L-function with confidence intervals and 
critical distance markers for interpretation. 
[Domain Knowledge]: 
Complete Spatial Randomness (CSR): a baseline model in spatial analysis, assuming a uniform, random 
distribution of flows across space. Deviations from CSR indicate clustering or dispersion, which 
the L-function quantifies. K-function: Measures the cumulative density of flows within a given 
distance, normalized by the total flow intensity. L-function: A transformed version of the K-function, 
making it easier to interpret deviations from CSR: L(r) > 0: Clustering, L(r)<0: Dispesion 
[Dataset Description]: 
dataset/flow.csv: Contains the geographical flows represented as connected origin-destination 
(OD) point pairs. 
Columns: x_o: X-coordinate of the origin point; y_o: Y-coordinate of the origin point; x_d: X-coordinate 
of the destination point; y_d: Y-coordinate of the destination point. 

dataset/o_area.csv: Contains the boundary coordinates for the region where the origin points 
lie. 
Columns: x: X-coordinate of the boundary point; y: Y-coordinate of the boundary point. 

dataset/d_area.csv: Contains the boundary coordinates for the region where the destination points 
lie. 
Columns: x: X-coordinate of the boundary point; y: Y-coordinate of the boundary point. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
32,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is to find the optimal path between mountain lion habitats given the local roads, land cover, 
elevations, and protected status data. You need to calculate the cost of elevation(ruggedness), 
distances to road and assign right values and reclassify land cover types and protected status. 
Then you will add them together with different weights. Finally, you will find the optimal region 
connections using the final cost layer and habitat information. Visualize and save the final output 
as ""optimal_path.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
32,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is to find the optimal path between mountain lion habitats given the local roads, land cover, 
elevations, and protected status data. You need to calculate the cost of elevation(ruggedness), 
distances to road and assign right values and reclassify land cover types and protected status. 
Then you will add them together with different weights. Finally, you will find the optimal region 
connections using the final cost layer and habitat information. Visualize and save the final output 
as ""optimal_path.png"". 
[Domain Knowledge]: 
Distance Accumulation: Arcgis Pro tool that calculates accumulated distance for each cell to sources. 
Focal Statistics: Arcgis Pro tool that calculates the statistics within a given area. Weighted 
Sum:The Raster Calculator tool allows you to create and run map algebra expressions in a tool. The 
map algebra expressions allow users to enter mathematical (addition, division, and so on) and logical 
(greater than, equal to, and so on) operators in the expression to operate on raster layers. The general 
structure of a map algebra statement is an assignment operator (=), which is used to separate the 
action to its right from the name of the output (a raster object) to its left. The input raster layers 
need to have same size (col, row). Optimal Region Connections:Arcgis tool that calculates optimal 
connectivity network between two or more input regions. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
32,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is to find the optimal path between mountain lion habitats given the local roads, land cover, 
elevations, and protected status data. You need to calculate the cost of elevation(ruggedness), 
distances to road and assign right values and reclassify land cover types and protected status. 
Then you will add them together with different weights. Finally, you will find the optimal region 
connections using the final cost layer and habitat information. Visualize and save the final output 
as ""optimal_path.png"". 
[Dataset Description]: 
dataset/landCover.tif: GeoTiff file storing the landcover types defined by National Land Cover 
Database 
Shape of dataset/landcover.tif: (2494, 3062) 

dataset/Protected_Status.tif: GeoTiff file storing the Protected status in 5 levels as well as 
the null data. 
Shape of dataset/Protected_Status.tif: (2494, 3062) 

dataset/Elevation.tif: GeoTiff file storing the elevation data. 
Shape of dataset/Protected_Status.tif: (2494, 3062) 

dataset/Roads.geojson: Geojson file storing major roads. 

dataset/habitat.geojson: Geojson file storing major habitats for mountain lions. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
32,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is to find the optimal path between mountain lion habitats given the local roads, land cover, 
elevations, and protected status data. You need to calculate the cost of elevation(ruggedness), 
distances to road and assign right values and reclassify land cover types and protected status. 
Then you will add them together with different weights. Finally, you will find the optimal region 
connections using the final cost layer and habitat information. Visualize and save the final output 
as ""optimal_path.png"". 
[Domain Knowledge]: 
Distance Accumulation: Arcgis Pro tool that calculates accumulated distance for each cell to sources. 
Focal Statistics: Arcgis Pro tool that calculates the statistics within a given area. Weighted 
Sum:The Raster Calculator tool allows you to create and run map algebra expressions in a tool. The 
map algebra expressions allow users to enter mathematical (addition, division, and so on) and logical 
(greater than, equal to, and so on) operators in the expression to operate on raster layers. The general 
structure of a map algebra statement is an assignment operator (=), which is used to separate the 
action to its right from the name of the output (a raster object) to its left. The input raster layers 
need to have same size (col, row). Optimal Region Connections:Arcgis tool that calculates optimal 
connectivity network between two or more input regions. 
[Dataset Description]: 
dataset/landCover.tif: GeoTiff file storing the landcover types defined by National Land Cover 
Database 
Shape of dataset/landcover.tif: (2494, 3062) 

dataset/Protected_Status.tif: GeoTiff file storing the Protected status in 5 levels as well as 
the null data. 
Shape of dataset/Protected_Status.tif: (2494, 3062) 

dataset/Elevation.tif: GeoTiff file storing the elevation data. 
Shape of dataset/Protected_Status.tif: (2494, 3062) 

dataset/Roads.geojson: Geojson file storing major roads. 

dataset/habitat.geojson: Geojson file storing major habitats for mountain lions. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
33,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the impacts of land subsidence on flooding. 
[Instruction]: 
Your task is analyzing the impact of land subsidence on flooding based on future elevation data of 
the study area. Your will calculate the flood damage to building in the future given the elevation 
of sea level in 2050 and shapefile of study area. You need find the flood depth given the elevation. 
Then you will filter and select the buildings within the flooded area. The final goal is summarizing 
the flood depth in each building area and calculate the final flood damage. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
33,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the impacts of land subsidence on flooding. 
[Instruction]: 
Your task is analyzing the impact of land subsidence on flooding based on future elevation data of 
the study area. Your will calculate the flood damage to building in the future given the elevation 
of sea level in 2050 and shapefile of study area. You need find the flood depth given the elevation. 
Then you will filter and select the buildings within the flooded area. The final goal is summarizing 
the flood depth in each building area and calculate the final flood damage. 
[Domain Knowledge]: 
Raster Calculator:Build and run a single map algebra expression using Python syntax. Raster to 
Polygon:Converts a raster dataset to polygon features. Select Layer By Location:Selects features 
based on a spatial relationship to features in another dataset or the same dataset. Summarize Within:Overlays 
a polygon layer with another layer to summarize the number of points, length of the lines, or area 
of the polygons within each polygon, and calculate attribute field statistics about the features 
within the polygons. Calculate Fied:Calculates the values of a field for a feature class, feature 
layer, or raster. Flood depth calculation equation: Int((""%flooded_area_2050%"" + 200)*-1) Cost 
Calculation Equation: if ($feature.MEAN_gridcode > 1) { (0.298 * (Log(0.01 * $feature.MEAN_gridcode)) 
+ 1.4502) * 271 * $feature.Shape_Area } else { 0 } 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
33,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the impacts of land subsidence on flooding. 
[Instruction]: 
Your task is analyzing the impact of land subsidence on flooding based on future elevation data of 
the study area. Your will calculate the flood damage to building in the future given the elevation 
of sea level in 2050 and shapefile of study area. You need find the flood depth given the elevation. 
Then you will filter and select the buildings within the flooded area. The final goal is summarizing 
the flood depth in each building area and calculate the final flood damage. 
[Dataset Description]: 
dataset/Elevation_2050.tif: Raster file for storing predicted flood evelation in 2050. 
data shape: (878, 1196) 

dataset/StudyAreaBuildings.shp: Polygon Shapefile for shape and features of buildings in the 
study area. 

Columns of dataset/StudyAreaBuildings.shp: 
'Shape_Leng', 'Shape_Area', 'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
33,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze the impacts of land subsidence on flooding. 
[Instruction]: 
Your task is analyzing the impact of land subsidence on flooding based on future elevation data of 
the study area. Your will calculate the flood damage to building in the future given the elevation 
of sea level in 2050 and shapefile of study area. You need find the flood depth given the elevation. 
Then you will filter and select the buildings within the flooded area. The final goal is summarizing 
the flood depth in each building area and calculate the final flood damage. 
[Domain Knowledge]: 
Raster Calculator:Build and run a single map algebra expression using Python syntax. Raster to 
Polygon:Converts a raster dataset to polygon features. Select Layer By Location:Selects features 
based on a spatial relationship to features in another dataset or the same dataset. Summarize Within:Overlays 
a polygon layer with another layer to summarize the number of points, length of the lines, or area 
of the polygons within each polygon, and calculate attribute field statistics about the features 
within the polygons. Calculate Fied:Calculates the values of a field for a feature class, feature 
layer, or raster. Flood depth calculation equation: Int((""%flooded_area_2050%"" + 200)*-1) Cost 
Calculation Equation: if ($feature.MEAN_gridcode > 1) { (0.298 * (Log(0.01 * $feature.MEAN_gridcode)) 
+ 1.4502) * 271 * $feature.Shape_Area } else { 0 } 
[Dataset Description]: 
dataset/Elevation_2050.tif: Raster file for storing predicted flood evelation in 2050. 
data shape: (878, 1196) 

dataset/StudyAreaBuildings.shp: Polygon Shapefile for shape and features of buildings in the 
study area. 

Columns of dataset/StudyAreaBuildings.shp: 
'Shape_Leng', 'Shape_Area', 'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
34,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Estimate the accessibility of roads to rural areas in Japan. 
[Instruction]: 
To promote sustainable development in the nation's rural areas, your task is estimating access 
to all-season roads. First, you'll add population and road data and limit your study area to rural 
regions. With the shape of rural area, you will clip the regional population of the rural areas.Then, 
you'll create a 2-kilometer buffer around all-season roads and clip the rural area out. The final 
goal is calculating the percentage of the rural population within that buffer to get the percentage 
of population in the rural area that has access to roads in 2km. Finally, visualize the population 
percentage in each rural area with a choropleth map. Save the result as ""pred_results/accessibility.png"". 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
34,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Estimate the accessibility of roads to rural areas in Japan. 
[Instruction]: 
To promote sustainable development in the nation's rural areas, your task is estimating access 
to all-season roads. First, you'll add population and road data and limit your study area to rural 
regions. With the shape of rural area, you will clip the regional population of the rural areas.Then, 
you'll create a 2-kilometer buffer around all-season roads and clip the rural area out. The final 
goal is calculating the percentage of the rural population within that buffer to get the percentage 
of population in the rural area that has access to roads in 2km. Finally, visualize the population 
percentage in each rural area with a choropleth map. Save the result as ""pred_results/accessibility.png"". 

[Domain Knowledge]: 
""Buffer"" creates buffer polygons or zones around input geometry features to a specified distance 
in GIS. Clip points, lines, or polygon geometries to the mask extent. ""Clip"" is used to cut out a piece 
of one map layer using one or more of the features in another layer. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
34,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Estimate the accessibility of roads to rural areas in Japan. 
[Instruction]: 
To promote sustainable development in the nation's rural areas, your task is estimating access 
to all-season roads. First, you'll add population and road data and limit your study area to rural 
regions. With the shape of rural area, you will clip the regional population of the rural areas.Then, 
you'll create a 2-kilometer buffer around all-season roads and clip the rural area out. The final 
goal is calculating the percentage of the rural population within that buffer to get the percentage 
of population in the rural area that has access to roads in 2km. Finally, visualize the population 
percentage in each rural area with a choropleth map. Save the result as ""pred_results/accessibility.png"". 

[Dataset Description]: 
dataset/ShikokuMetropolitan.geojson: Geojson file that stores the shape and features for towns 
in Shikoku area of Japan as polygons. Area type indicate whether an area is rural. 

Key Fields: 'OBJECTID', 'JISCODE', 'PNAME', 'MEACODE', 'MEANAME', 'AREANAME', 
'AREATYPE', 'Shape__Area', 'Shape__Length', 'geometry' 

dataset/AllSeasonRoads.geojson: Geojson file that stores all season emergency roads in Shikoku 
area as lines. 

Key Fields: 'OBJECTID', 'PrefectureNum', 'EmergencyRoadCategory', 'Road', 'ER_Road', 
'Dependency', 'LastUpdated', 'Shape__Length', 'geometry' 

dataset/ShikokuPopulation.geojson: Geojson file that stores the shape and features for areas 
in Shikoku. D0001 columns refers to population of that area in 2015. 

Key Fields: 'OBJECTID', 'ID', 'D0001', 'Shape__Area', 'Shape__Length', 'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
34,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Estimate the accessibility of roads to rural areas in Japan. 
[Instruction]: 
To promote sustainable development in the nation's rural areas, your task is estimating access 
to all-season roads. First, you'll add population and road data and limit your study area to rural 
regions. With the shape of rural area, you will clip the regional population of the rural areas.Then, 
you'll create a 2-kilometer buffer around all-season roads and clip the rural area out. The final 
goal is calculating the percentage of the rural population within that buffer to get the percentage 
of population in the rural area that has access to roads in 2km. Finally, visualize the population 
percentage in each rural area with a choropleth map. Save the result as ""pred_results/accessibility.png"". 

[Domain Knowledge]: 
""Buffer"" creates buffer polygons or zones around input geometry features to a specified distance 
in GIS. Clip points, lines, or polygon geometries to the mask extent. ""Clip"" is used to cut out a piece 
of one map layer using one or more of the features in another layer. 
[Dataset Description]: 
dataset/ShikokuMetropolitan.geojson: Geojson file that stores the shape and features for towns 
in Shikoku area of Japan as polygons. Area type indicate whether an area is rural. 

Key Fields: 'OBJECTID', 'JISCODE', 'PNAME', 'MEACODE', 'MEANAME', 'AREANAME', 
'AREATYPE', 'Shape__Area', 'Shape__Length', 'geometry' 

dataset/AllSeasonRoads.geojson: Geojson file that stores all season emergency roads in Shikoku 
area as lines. 

Key Fields: 'OBJECTID', 'PrefectureNum', 'EmergencyRoadCategory', 'Road', 'ER_Road', 
'Dependency', 'LastUpdated', 'Shape__Length', 'geometry' 

dataset/ShikokuPopulation.geojson: Geojson file that stores the shape and features for areas 
in Shikoku. D0001 columns refers to population of that area in 2015. 

Key Fields: 'OBJECTID', 'ID', 'D0001', 'Shape__Area', 'Shape__Length', 'geometry' 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
35,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate landslide potential for communities affected by wildfires 
[Instruction]: 
Your task is doing raster analysis to analyze burn severity, slope and landcover in Santa Rosa, California, 
region, which was devastated by a wildfire in 2017. You have to calculate the normalized Burn ratio 
before and after the wildfire using two landsat 8 satellite imageries. Subtracting the resulted 
layers to get burn severity. Using the elevation layer to find the slope of the map. For the burn severity, 
slope, and landcover, remapping into 5 categories, from low to high. Finally, weighted overlaying 
them together and save them as one landslide risk map as 'pred_results/landslide_map.png' 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
35,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate landslide potential for communities affected by wildfires 
[Instruction]: 
Your task is doing raster analysis to analyze burn severity, slope and landcover in Santa Rosa, California, 
region, which was devastated by a wildfire in 2017. You have to calculate the normalized Burn ratio 
before and after the wildfire using two landsat 8 satellite imageries. Subtracting the resulted 
layers to get burn severity. Using the elevation layer to find the slope of the map. For the burn severity, 
slope, and landcover, remapping into 5 categories, from low to high. Finally, weighted overlaying 
them together and save them as one landslide risk map as 'pred_results/landslide_map.png' 
[Domain Knowledge]: 
Normalized Burn Ratio (NBR) is used to identify burned areas and provide a measure of burn severity. 
NBR ranges between -1 and 1. A high NBR value indicates healthy vegetation. A low value indicates 
bare ground and recently burnt areas.It is calculated as a ratio between the NIR and SWIR values in 
traditional fashion. In Landsat 8-9, NBR = (Band 5 - Band 7) / (Band 5 + Band 7). Burn Severity: NBR after 
wildfire Minus before wildfire Slope:The Slope tool identifies the steepness at each cell of a raster 
surface. The lower the slope value, the flatter the terrain; the higher the slope value, the steeper 
the terrain. Remap: Reclassification process that normalizes different raster files to the same 
range. Weighted Overlay: Each raster layer's value range should be rescale to the same range. Each 
raster layer is weighted differently: Landcover(15%), Burn Severity(30%), Slope(55%). Reclassification 
for data should be in the range of 0 to 5 Reclassification for landcover: < 12 1 21 - 22 3 23 4 24 - 31 5 41 
- 44 1 52 3 71 4 81 3 82 4 90 - 95 2 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
35,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate landslide potential for communities affected by wildfires 
[Instruction]: 
Your task is doing raster analysis to analyze burn severity, slope and landcover in Santa Rosa, California, 
region, which was devastated by a wildfire in 2017. You have to calculate the normalized Burn ratio 
before and after the wildfire using two landsat 8 satellite imageries. Subtracting the resulted 
layers to get burn severity. Using the elevation layer to find the slope of the map. For the burn severity, 
slope, and landcover, remapping into 5 categories, from low to high. Finally, weighted overlaying 
them together and save them as one landslide risk map as 'pred_results/landslide_map.png' 
[Dataset Description]: 
dataset/DEM_30m.tif: Santa Rosa's elevation in 30 meters spatial resolution. 

dataset/Before_L8.tif: Landsat 8 satellite imageries before 2017 wildfire. 

dataset/After_L8.tif: Landsat 8 satellite imageries after 2017 wildfire. 

dataset/Sonoma_NLCD2011.tif: Landcover status of Santa Rosa area published by NLCD in 2011. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
35,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate landslide potential for communities affected by wildfires 
[Instruction]: 
Your task is doing raster analysis to analyze burn severity, slope and landcover in Santa Rosa, California, 
region, which was devastated by a wildfire in 2017. You have to calculate the normalized Burn ratio 
before and after the wildfire using two landsat 8 satellite imageries. Subtracting the resulted 
layers to get burn severity. Using the elevation layer to find the slope of the map. For the burn severity, 
slope, and landcover, remapping into 5 categories, from low to high. Finally, weighted overlaying 
them together and save them as one landslide risk map as 'pred_results/landslide_map.png' 
[Domain Knowledge]: 
Normalized Burn Ratio (NBR) is used to identify burned areas and provide a measure of burn severity. 
NBR ranges between -1 and 1. A high NBR value indicates healthy vegetation. A low value indicates 
bare ground and recently burnt areas.It is calculated as a ratio between the NIR and SWIR values in 
traditional fashion. In Landsat 8-9, NBR = (Band 5 - Band 7) / (Band 5 + Band 7). Burn Severity: NBR after 
wildfire Minus before wildfire Slope:The Slope tool identifies the steepness at each cell of a raster 
surface. The lower the slope value, the flatter the terrain; the higher the slope value, the steeper 
the terrain. Remap: Reclassification process that normalizes different raster files to the same 
range. Weighted Overlay: Each raster layer's value range should be rescale to the same range. Each 
raster layer is weighted differently: Landcover(15%), Burn Severity(30%), Slope(55%). Reclassification 
for data should be in the range of 0 to 5 Reclassification for landcover: < 12 1 21 - 22 3 23 4 24 - 31 5 41 
- 44 1 52 3 71 4 81 3 82 4 90 - 95 2 
[Dataset Description]: 
dataset/DEM_30m.tif: Santa Rosa's elevation in 30 meters spatial resolution. 

dataset/Before_L8.tif: Landsat 8 satellite imageries before 2017 wildfire. 

dataset/After_L8.tif: Landsat 8 satellite imageries after 2017 wildfire. 

dataset/Sonoma_NLCD2011.tif: Landcover status of Santa Rosa area published by NLCD in 2011. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
36,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Compute the change in vegetation before and after a hailstorm with the SAVI index 
[Instruction]: 
Your task is to assess the impact of hail damage on cornfields using pre- and post-storm satellite 
imagery. You will calculate vegetation loss due to the storm, leveraging the Soil-Adjusted Vegetation 
Index (SAVI) applied to multispectral imagery from the study area. The process involves determining 
vegetation health before and after the storm, identifying changes at the pixel level, and analyzing 
the average vegetation loss for each crop field. The ultimate goal is to summarize the extent of damage 
for each field, offering valuable insights for agricultural recovery efforts. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
36,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Compute the change in vegetation before and after a hailstorm with the SAVI index 
[Instruction]: 
Your task is to assess the impact of hail damage on cornfields using pre- and post-storm satellite 
imagery. You will calculate vegetation loss due to the storm, leveraging the Soil-Adjusted Vegetation 
Index (SAVI) applied to multispectral imagery from the study area. The process involves determining 
vegetation health before and after the storm, identifying changes at the pixel level, and analyzing 
the average vegetation loss for each crop field. The ultimate goal is to summarize the extent of damage 
for each field, offering valuable insights for agricultural recovery efforts. 
[Domain Knowledge]: 
SAVI is particularly useful in areas with intermediate vegetation cover and is computed using the 
formula: SAVI=((NIR-Red)/(NIR+Red+L)) * (1+L), where L is a correction factor (commonly set to 
0.5). Raster-based analysis allows pixel-by-pixel calculations, such as vegetation indices, 
and is fundamental for spatial quantification of phenomena like crop damage. Zonal Statistics 
aggregates raster data based on defined zones (e.g., polygons of field boundaries). Metrics such 
as the mean loss of vegetation can provide valuable insights at the field level. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
36,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Compute the change in vegetation before and after a hailstorm with the SAVI index 
[Instruction]: 
Your task is to assess the impact of hail damage on cornfields using pre- and post-storm satellite 
imagery. You will calculate vegetation loss due to the storm, leveraging the Soil-Adjusted Vegetation 
Index (SAVI) applied to multispectral imagery from the study area. The process involves determining 
vegetation health before and after the storm, identifying changes at the pixel level, and analyzing 
the average vegetation loss for each crop field. The ultimate goal is to summarize the extent of damage 
for each field, offering valuable insights for agricultural recovery efforts. 
[Dataset Description]: 
dataset/Before_Storm.tif: Raster file containing multispectral satellite imagery before the 
hailstorm. Data shape: (9030, 9121) 

dataset/After_Storm.tif: Raster file containing multispectral satellite imagery after the 
hailstorm. data shape: (7293,8549) 


dataset/Field_Boundaries.shp: Polygon shapefile representing the boundaries of cultivated 
fields in the study area. columns: 'Field_ID', 'Shape_Area', 'geometry' 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
36,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Compute the change in vegetation before and after a hailstorm with the SAVI index 
[Instruction]: 
Your task is to assess the impact of hail damage on cornfields using pre- and post-storm satellite 
imagery. You will calculate vegetation loss due to the storm, leveraging the Soil-Adjusted Vegetation 
Index (SAVI) applied to multispectral imagery from the study area. The process involves determining 
vegetation health before and after the storm, identifying changes at the pixel level, and analyzing 
the average vegetation loss for each crop field. The ultimate goal is to summarize the extent of damage 
for each field, offering valuable insights for agricultural recovery efforts. 
[Domain Knowledge]: 
SAVI is particularly useful in areas with intermediate vegetation cover and is computed using the 
formula: SAVI=((NIR-Red)/(NIR+Red+L)) * (1+L), where L is a correction factor (commonly set to 
0.5). Raster-based analysis allows pixel-by-pixel calculations, such as vegetation indices, 
and is fundamental for spatial quantification of phenomena like crop damage. Zonal Statistics 
aggregates raster data based on defined zones (e.g., polygons of field boundaries). Metrics such 
as the mean loss of vegetation can provide valuable insights at the field level. 
[Dataset Description]: 
dataset/Before_Storm.tif: Raster file containing multispectral satellite imagery before the 
hailstorm. Data shape: (9030, 9121) 

dataset/After_Storm.tif: Raster file containing multispectral satellite imagery after the 
hailstorm. data shape: (7293,8549) 


dataset/Field_Boundaries.shp: Polygon shapefile representing the boundaries of cultivated 
fields in the study area. columns: 'Field_ID', 'Shape_Area', 'geometry' 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
37,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze of human sentiments of heat exposure using social media data 
[Instruction]: 
Your task is to do a national-scale analysis of human sentiments of heat exposure using location-based 
social media Twitter data. Based on a heat vocab dictionary file, you will construct a dictionary 
to store the values of each vocabulary. You will retrieve all weather related tweets from the loaded 
json file. Then you will assign the tweets to counties in USA. You will count the number of tweet to 
sum their values and find the mean to be the heat exposure value. You will also have to normalize the 
heat exposure values with max and min. Finally, visualize the results in a choropleth map using natural 
break and save the results as ""pred_results/US_heat_exposure_map.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
37,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze of human sentiments of heat exposure using social media data 
[Instruction]: 
Your task is to do a national-scale analysis of human sentiments of heat exposure using location-based 
social media Twitter data. Based on a heat vocab dictionary file, you will construct a dictionary 
to store the values of each vocabulary. You will retrieve all weather related tweets from the loaded 
json file. Then you will assign the tweets to counties in USA. You will count the number of tweet to 
sum their values and find the mean to be the heat exposure value. You will also have to normalize the 
heat exposure values with max and min. Finally, visualize the results in a choropleth map using natural 
break and save the results as ""pred_results/US_heat_exposure_map.png"". 
[Domain Knowledge]: 
Human Sentiments: Emotions, opinions, and attitude from posts on social media platform, which 
reflects people's sentiments. Heat Exposure：Extreme heat related weather condition. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
37,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze of human sentiments of heat exposure using social media data 
[Instruction]: 
Your task is to do a national-scale analysis of human sentiments of heat exposure using location-based 
social media Twitter data. Based on a heat vocab dictionary file, you will construct a dictionary 
to store the values of each vocabulary. You will retrieve all weather related tweets from the loaded 
json file. Then you will assign the tweets to counties in USA. You will count the number of tweet to 
sum their values and find the mean to be the heat exposure value. You will also have to normalize the 
heat exposure values with max and min. Finally, visualize the results in a choropleth map using natural 
break and save the results as ""pred_results/US_heat_exposure_map.png"". 
[Dataset Description]: 
dataset/data20000.txt: Stores 20000 weather related vocabularies along with their sentiment 
scores in the form of ""term: 0.42"". 

dataset/12500-tweets-2021-09-24_01-48-23.json: Json file that stores 12500 tweets in United 
states on 2021/09/25. Some of the tweets are geotagged that it has longitudes and latitudes. 

dataset/cb_2020_us_county_20m.shp: County Shapefiles for USA. It has polygon geometry for each 
county. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
37,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Analyze of human sentiments of heat exposure using social media data 
[Instruction]: 
Your task is to do a national-scale analysis of human sentiments of heat exposure using location-based 
social media Twitter data. Based on a heat vocab dictionary file, you will construct a dictionary 
to store the values of each vocabulary. You will retrieve all weather related tweets from the loaded 
json file. Then you will assign the tweets to counties in USA. You will count the number of tweet to 
sum their values and find the mean to be the heat exposure value. You will also have to normalize the 
heat exposure values with max and min. Finally, visualize the results in a choropleth map using natural 
break and save the results as ""pred_results/US_heat_exposure_map.png"". 
[Domain Knowledge]: 
Human Sentiments: Emotions, opinions, and attitude from posts on social media platform, which 
reflects people's sentiments. Heat Exposure：Extreme heat related weather condition. 
[Dataset Description]: 
dataset/data20000.txt: Stores 20000 weather related vocabularies along with their sentiment 
scores in the form of ""term: 0.42"". 

dataset/12500-tweets-2021-09-24_01-48-23.json: Json file that stores 12500 tweets in United 
states on 2021/09/25. Some of the tweets are geotagged that it has longitudes and latitudes. 

dataset/cb_2020_us_county_20m.shp: County Shapefiles for USA. It has polygon geometry for each 
county. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
38,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate travel time from one location to others in a neighborhood. 
[Instruction]: 
Your task is to calculate the travel time from Natural History Building(Geography department) 
at University of Illinois Urbana-Champaign to the other buildings on the campus. You will retreive 
the place information as graph from Open Street Map networkdata. Using the graph, you will find out 
the speeds and required travel time between nodes. Calculate the driving-time distances for the 
location nodes from the nearest node next to Natural History Building. Set the node attributes with 
the travel time. Finally, visualize the travel time by plotting the nodes with specific colors and 
save the graph as ""pred_results/travel_time.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
38,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate travel time from one location to others in a neighborhood. 
[Instruction]: 
Your task is to calculate the travel time from Natural History Building(Geography department) 
at University of Illinois Urbana-Champaign to the other buildings on the campus. You will retreive 
the place information as graph from Open Street Map networkdata. Using the graph, you will find out 
the speeds and required travel time between nodes. Calculate the driving-time distances for the 
location nodes from the nearest node next to Natural History Building. Set the node attributes with 
the travel time. Finally, visualize the travel time by plotting the nodes with specific colors and 
save the graph as ""pred_results/travel_time.png"". 
[Domain Knowledge]: 
Travel-time Calculation: The amount of time traveling from one place to another. It can be calculated 
by using add_edge_speed and add_edge_travel_times functions in osmnx package, which return the 
speed limit and total travel time between nodes. OSM Network data: osmnx.graph_from_address function 
returns the network graph around a specific location. In this case, the address is ""1301 W Green St, 
Urbana, IL 61801"" and the network_type should be ""drive"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
38,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate travel time from one location to others in a neighborhood. 
[Instruction]: 
Your task is to calculate the travel time from Natural History Building(Geography department) 
at University of Illinois Urbana-Champaign to the other buildings on the campus. You will retreive 
the place information as graph from Open Street Map networkdata. Using the graph, you will find out 
the speeds and required travel time between nodes. Calculate the driving-time distances for the 
location nodes from the nearest node next to Natural History Building. Set the node attributes with 
the travel time. Finally, visualize the travel time by plotting the nodes with specific colors and 
save the graph as ""pred_results/travel_time.png"". 
[Dataset Description]: 
nan
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
38,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate travel time from one location to others in a neighborhood. 
[Instruction]: 
Your task is to calculate the travel time from Natural History Building(Geography department) 
at University of Illinois Urbana-Champaign to the other buildings on the campus. You will retreive 
the place information as graph from Open Street Map networkdata. Using the graph, you will find out 
the speeds and required travel time between nodes. Calculate the driving-time distances for the 
location nodes from the nearest node next to Natural History Building. Set the node attributes with 
the travel time. Finally, visualize the travel time by plotting the nodes with specific colors and 
save the graph as ""pred_results/travel_time.png"". 
[Domain Knowledge]: 
Travel-time Calculation: The amount of time traveling from one place to another. It can be calculated 
by using add_edge_speed and add_edge_travel_times functions in osmnx package, which return the 
speed limit and total travel time between nodes. OSM Network data: osmnx.graph_from_address function 
returns the network graph around a specific location. In this case, the address is ""1301 W Green St, 
Urbana, IL 61801"" and the network_type should be ""drive"". 
[Dataset Description]: 
nan
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
39,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Train a Geographically Weighted Regression model to predict Georgia's Bachelor's degree rate. 

[Instruction]: 
Your task is to train a Geographically Weighted Regression(GWR) model with demographical data 
in the state of Georgia. The target variable is the percentage of bachelor's degree in each county 
while the indepdent variables are the percentage of foreign-born population, black population, 
and rural populations. You should search for the optimal bandwidth for the GWR models and plot their 
differences in the same graph. Save the plots as ""pred_results/GA_GWR.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
39,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Train a Geographically Weighted Regression model to predict Georgia's Bachelor's degree rate. 

[Instruction]: 
Your task is to train a Geographically Weighted Regression(GWR) model with demographical data 
in the state of Georgia. The target variable is the percentage of bachelor's degree in each county 
while the indepdent variables are the percentage of foreign-born population, black population, 
and rural populations. You should search for the optimal bandwidth for the GWR models and plot their 
differences in the same graph. Save the plots as ""pred_results/GA_GWR.png"". 
[Domain Knowledge]: 
Geographically Weighted Regression Model: A spatial technique that analyzes the relationship 
between variables by considering their geographical locations. It can be used in python by Multiscale 
Geographically Weighted Regression (MGWR) packages. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
39,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Train a Geographically Weighted Regression model to predict Georgia's Bachelor's degree rate. 

[Instruction]: 
Your task is to train a Geographically Weighted Regression(GWR) model with demographical data 
in the state of Georgia. The target variable is the percentage of bachelor's degree in each county 
while the indepdent variables are the percentage of foreign-born population, black population, 
and rural populations. You should search for the optimal bandwidth for the GWR models and plot their 
differences in the same graph. Save the plots as ""pred_results/GA_GWR.png"". 
[Dataset Description]: 
nan
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
39,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Train a Geographically Weighted Regression model to predict Georgia's Bachelor's degree rate. 

[Instruction]: 
Your task is to train a Geographically Weighted Regression(GWR) model with demographical data 
in the state of Georgia. The target variable is the percentage of bachelor's degree in each county 
while the indepdent variables are the percentage of foreign-born population, black population, 
and rural populations. You should search for the optimal bandwidth for the GWR models and plot their 
differences in the same graph. Save the plots as ""pred_results/GA_GWR.png"". 
[Domain Knowledge]: 
Geographically Weighted Regression Model: A spatial technique that analyzes the relationship 
between variables by considering their geographical locations. It can be used in python by Multiscale 
Geographically Weighted Regression (MGWR) packages. 
[Dataset Description]: 
nan
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
40,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate and visualize changes in malaria prevalence 
[Instruction]: 
Your task is to analyze changes in malaria prevalence among children aged 2–10 in Sub-Saharan Africa 
from 2000 to 2015. Using raster data from the Malaria Atlas Project, calculate the difference between 
the prevalence rates in 2000 and 2015 using the Minus raster function. Then, visualize the change 
in malaria prevalence with a diverging color scheme and adjust the symbology to highlight areas 
of increase and decrease. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
40,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate and visualize changes in malaria prevalence 
[Instruction]: 
Your task is to analyze changes in malaria prevalence among children aged 2–10 in Sub-Saharan Africa 
from 2000 to 2015. Using raster data from the Malaria Atlas Project, calculate the difference between 
the prevalence rates in 2000 and 2015 using the Minus raster function. Then, visualize the change 
in malaria prevalence with a diverging color scheme and adjust the symbology to highlight areas 
of increase and decrease. 
[Domain Knowledge]: 
Malaria Prevalence Analysis: Understanding malaria's spatial distribution and temporal changes, 
especially among children in Sub-Saharan Africa. Raster Analysis Techniques: Applying raster 
functions (e.g., Minus) to calculate changes over time. Remapping raster values into meaningful 
categories for analysis and visualization. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
40,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate and visualize changes in malaria prevalence 
[Instruction]: 
Your task is to analyze changes in malaria prevalence among children aged 2–10 in Sub-Saharan Africa 
from 2000 to 2015. Using raster data from the Malaria Atlas Project, calculate the difference between 
the prevalence rates in 2000 and 2015 using the Minus raster function. Then, visualize the change 
in malaria prevalence with a diverging color scheme and adjust the symbology to highlight areas 
of increase and decrease. 
[Dataset Description]: 
dataset/2015_Nature_Africa_PR.2000.tif: GeoTIFF file storing malaria prevalence rates among 
children aged 2–10 in Sub-Saharan Africa for the year 2000. Data shape: (1741, 1681) 

dataset/2015_Nature_Africa_PR.2015.tif: GeoTIFF file storing malaria prevalence rates among 
children aged 2–10 in Sub-Saharan Africa for the year 2015. Data shape:(1741, 1681) 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
40,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate and visualize changes in malaria prevalence 
[Instruction]: 
Your task is to analyze changes in malaria prevalence among children aged 2–10 in Sub-Saharan Africa 
from 2000 to 2015. Using raster data from the Malaria Atlas Project, calculate the difference between 
the prevalence rates in 2000 and 2015 using the Minus raster function. Then, visualize the change 
in malaria prevalence with a diverging color scheme and adjust the symbology to highlight areas 
of increase and decrease. 
[Domain Knowledge]: 
Malaria Prevalence Analysis: Understanding malaria's spatial distribution and temporal changes, 
especially among children in Sub-Saharan Africa. Raster Analysis Techniques: Applying raster 
functions (e.g., Minus) to calculate changes over time. Remapping raster values into meaningful 
categories for analysis and visualization. 
[Dataset Description]: 
dataset/2015_Nature_Africa_PR.2000.tif: GeoTIFF file storing malaria prevalence rates among 
children aged 2–10 in Sub-Saharan Africa for the year 2000. Data shape: (1741, 1681) 

dataset/2015_Nature_Africa_PR.2015.tif: GeoTIFF file storing malaria prevalence rates among 
children aged 2–10 in Sub-Saharan Africa for the year 2015. Data shape:(1741, 1681) 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
41,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Improve campsite data quality using a relationship class 
[Instruction]: 
In this tutorial, you are a regional GIS specialist tasked with helping improve campsite data quality 
in Wyoming. You'll initially address the loss of data at the park level by creating joins and relates 
to incorporate associated records then inspect and create a relationship class between campsites 
and campgrounds. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
41,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Improve campsite data quality using a relationship class 
[Instruction]: 
In this tutorial, you are a regional GIS specialist tasked with helping improve campsite data quality 
in Wyoming. You'll initially address the loss of data at the park level by creating joins and relates 
to incorporate associated records then inspect and create a relationship class between campsites 
and campgrounds. 
[Domain Knowledge]: 
Relate, a temporary RDBMS mechanism that links tables via key fields without merging them, enabling 
a campsite to access multiple dynamic attributes. To make Relate persistent, create a relationship 
class in a geodatabase. Create Relationship Class is to store an association between fields or features 
in the origin table and the destination table. It is created with one-to-one, one-to-many, or many-to-many 
cardinality. Add Rule To Realtionship Class: Once a rule is added to a relationship class, that rule 
becomes the only valid relationship that can exist. To make other relationship combinations and 
cardinalities valid, additional relationship rules must be added. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
41,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Improve campsite data quality using a relationship class 
[Instruction]: 
In this tutorial, you are a regional GIS specialist tasked with helping improve campsite data quality 
in Wyoming. You'll initially address the loss of data at the park level by creating joins and relates 
to incorporate associated records then inspect and create a relationship class between campsites 
and campgrounds. 
[Dataset Description]: 

dataset/National Parks: layer file for national park boundaries, polygon 
dataset/CampsiteAttributes.csv: Different features for each camp site. 
Key Columns: ""AttributeID"", ""AttributeName"", ""AttributeValue"", ""EntityID"", ""EntityType"" 

dataset/OrganizationsTable.csv: Organization informations. Key Columns: ""OrgID"", ""OrgType"", 
""OrgName"", ""OrgImageURL"","" OrgURLAddress"", ""OrgAbbrevName"", ""OrgJurisdictionType"", ""OrgParentID"", 
""LastUpdatedDate"" 
dataset/Campsites: layer file for campsites, point 
dataset/Facilities: layer file for facilities' locations, point 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
41,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Improve campsite data quality using a relationship class 
[Instruction]: 
In this tutorial, you are a regional GIS specialist tasked with helping improve campsite data quality 
in Wyoming. You'll initially address the loss of data at the park level by creating joins and relates 
to incorporate associated records then inspect and create a relationship class between campsites 
and campgrounds. 
[Domain Knowledge]: 
Relate, a temporary RDBMS mechanism that links tables via key fields without merging them, enabling 
a campsite to access multiple dynamic attributes. To make Relate persistent, create a relationship 
class in a geodatabase. Create Relationship Class is to store an association between fields or features 
in the origin table and the destination table. It is created with one-to-one, one-to-many, or many-to-many 
cardinality. Add Rule To Realtionship Class: Once a rule is added to a relationship class, that rule 
becomes the only valid relationship that can exist. To make other relationship combinations and 
cardinalities valid, additional relationship rules must be added. 
[Dataset Description]: 

dataset/National Parks: layer file for national park boundaries, polygon 
dataset/CampsiteAttributes.csv: Different features for each camp site. 
Key Columns: ""AttributeID"", ""AttributeName"", ""AttributeValue"", ""EntityID"", ""EntityType"" 

dataset/OrganizationsTable.csv: Organization informations. Key Columns: ""OrgID"", ""OrgType"", 
""OrgName"", ""OrgImageURL"","" OrgURLAddress"", ""OrgAbbrevName"", ""OrgJurisdictionType"", ""OrgParentID"", 
""LastUpdatedDate"" 
dataset/Campsites: layer file for campsites, point 
dataset/Facilities: layer file for facilities' locations, point 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
42,workflow,False,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Investigate spatial patterns for Airbnb prices in Berlin 
[Instruction]: 
Your task is to perform spatial autocorrelation analysis on airbnb prices in Berlin using Python. 
The analysis involves loading geospatial data of Berlin neighborhoods and real estate listings, 
and then converting to GeoDataFrame. You will perform a spatial join to associate listings with 
neighborhoods and calculate the median price for each neighborhood group. After handling missing 
data, you will analyze spatial similarity by creating a spatial weights matrix based on their median 
prices. The Local Moran's I statistic will be used to identify significant cold spots and hot spots. 
Finally, you will visualize these spatial patterns on a map, highlighting areas with significant 
spatial autocorrelation, and save the output as ""moran_local.png"". 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
42,workflow,True,False,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Investigate spatial patterns for Airbnb prices in Berlin 
[Instruction]: 
Your task is to perform spatial autocorrelation analysis on airbnb prices in Berlin using Python. 
The analysis involves loading geospatial data of Berlin neighborhoods and real estate listings, 
and then converting to GeoDataFrame. You will perform a spatial join to associate listings with 
neighborhoods and calculate the median price for each neighborhood group. After handling missing 
data, you will analyze spatial similarity by creating a spatial weights matrix based on their median 
prices. The Local Moran's I statistic will be used to identify significant cold spots and hot spots. 
Finally, you will visualize these spatial patterns on a map, highlighting areas with significant 
spatial autocorrelation, and save the output as ""moran_local.png"". 
[Domain Knowledge]: 
esda package: ESDA is an open-source Python library for the exploratory analysis of spatial data. 
Spatial Similarity: Using queen contiguity to calculate spatial weight, which can be done by lps.weights.Queen 
function. Moran's I: Measures spatial autocorrelation based on feature locations and attribute 
values using the Moran's I statistic. Can be calculated by esda.moran.Moran_Local function. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
42,workflow,False,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Investigate spatial patterns for Airbnb prices in Berlin 
[Instruction]: 
Your task is to perform spatial autocorrelation analysis on airbnb prices in Berlin using Python. 
The analysis involves loading geospatial data of Berlin neighborhoods and real estate listings, 
and then converting to GeoDataFrame. You will perform a spatial join to associate listings with 
neighborhoods and calculate the median price for each neighborhood group. After handling missing 
data, you will analyze spatial similarity by creating a spatial weights matrix based on their median 
prices. The Local Moran's I statistic will be used to identify significant cold spots and hot spots. 
Finally, you will visualize these spatial patterns on a map, highlighting areas with significant 
spatial autocorrelation, and save the output as ""moran_local.png"". 
[Dataset Description]: 
dataset/berling_neighbourhoods.geojson:Geojson file for multipolygons of neighbourhoods 
in Berling, properties include ""neighbourhood"" and ""neighbourhood_group"". 

dataset/berlin-listings.csv: Csv file of Berling Airbnb information, with lat and lng of Airbnb. 

Key Columns: id scrape_id city state zipcode smart_location country_code country latitude longitude 
price 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
42,workflow,True,True,False,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Investigate spatial patterns for Airbnb prices in Berlin 
[Instruction]: 
Your task is to perform spatial autocorrelation analysis on airbnb prices in Berlin using Python. 
The analysis involves loading geospatial data of Berlin neighborhoods and real estate listings, 
and then converting to GeoDataFrame. You will perform a spatial join to associate listings with 
neighborhoods and calculate the median price for each neighborhood group. After handling missing 
data, you will analyze spatial similarity by creating a spatial weights matrix based on their median 
prices. The Local Moran's I statistic will be used to identify significant cold spots and hot spots. 
Finally, you will visualize these spatial patterns on a map, highlighting areas with significant 
spatial autocorrelation, and save the output as ""moran_local.png"". 
[Domain Knowledge]: 
esda package: ESDA is an open-source Python library for the exploratory analysis of spatial data. 
Spatial Similarity: Using queen contiguity to calculate spatial weight, which can be done by lps.weights.Queen 
function. Moran's I: Measures spatial autocorrelation based on feature locations and attribute 
values using the Moran's I statistic. Can be calculated by esda.moran.Moran_Local function. 
[Dataset Description]: 
dataset/berling_neighbourhoods.geojson:Geojson file for multipolygons of neighbourhoods 
in Berling, properties include ""neighbourhood"" and ""neighbourhood_group"". 

dataset/berlin-listings.csv: Csv file of Berling Airbnb information, with lat and lng of Airbnb. 

Key Columns: id scrape_id city state zipcode smart_location country_code country latitude longitude 
price 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
43,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Use animal GPS tracks to model home range to understand where they are and how they move over time. 

[Instruction]: 
Your task is to analyze and visualize elk movements using the provided dataset. The goal is to estimate 
home ranges and assess habitat preferences using spatial analysis techniques, including Minimum 
Bounding Geometry (Convex Hull), Kernel Density Estimation, and Density-Based Clustering (DBSCAN). 
The analysis will generate spatial outputs stored in ""dataset/elk_home_range.gdb"" and ""dataset/"". 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
43,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Use animal GPS tracks to model home range to understand where they are and how they move over time. 

[Instruction]: 
Your task is to analyze and visualize elk movements using the provided dataset. The goal is to estimate 
home ranges and assess habitat preferences using spatial analysis techniques, including Minimum 
Bounding Geometry (Convex Hull), Kernel Density Estimation, and Density-Based Clustering (DBSCAN). 
The analysis will generate spatial outputs stored in ""dataset/elk_home_range.gdb"" and ""dataset/"". 

[Domain Knowledge]: 
""Home range"" can be defined as the area within which an animal normally lives and finds what it needs 
for survival. Basically, the home range is the area that an animal travels for its normal daily activities. 
""Minimum Bounding Geometry"" creates a feature class containing polygons which represent a specified 
minimum bounding geometry enclosing each input feature or each group of input features. ""Convex 
hull"" is the smallest convex polygon that can enclose a group of objects, such as a group of points. 
""Kernel Density Mapping"" calculates and visualizes features's density in a given area. ""DBSCAN"", 
Density-Based Spatial Clustering of Applications with Noise that cluster the points based on density 
criterion. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
43,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Use animal GPS tracks to model home range to understand where they are and how they move over time. 

[Instruction]: 
Your task is to analyze and visualize elk movements using the provided dataset. The goal is to estimate 
home ranges and assess habitat preferences using spatial analysis techniques, including Minimum 
Bounding Geometry (Convex Hull), Kernel Density Estimation, and Density-Based Clustering (DBSCAN). 
The analysis will generate spatial outputs stored in ""dataset/elk_home_range.gdb"" and ""dataset/"". 

[Dataset Description]: 
dataset/Elk_in_Southwestern_Alberta_2009.geojson: geojson files for storing points of Elk 
movements in Southwestern Alberta 2009. 

Columns of dataset/Elk_in_Southwestern_Alberta_2009.geojson: 
'OBJECTID', 'timestamp', 'long', 'lat', 'comments', 'external_t', 'dop', 
'fix_type_r', 'satellite_', 'height', 'crc_status', 'outlier_ma', 
'sensor_typ', 'individual', 'tag_ident', 'ind_ident', 'study_name', 
'date', 'time', 'timestamp_Converted', 'summer_indicator', 'geometry' 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
43,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Use animal GPS tracks to model home range to understand where they are and how they move over time. 

[Instruction]: 
Your task is to analyze and visualize elk movements using the provided dataset. The goal is to estimate 
home ranges and assess habitat preferences using spatial analysis techniques, including Minimum 
Bounding Geometry (Convex Hull), Kernel Density Estimation, and Density-Based Clustering (DBSCAN). 
The analysis will generate spatial outputs stored in ""dataset/elk_home_range.gdb"" and ""dataset/"". 

[Domain Knowledge]: 
""Home range"" can be defined as the area within which an animal normally lives and finds what it needs 
for survival. Basically, the home range is the area that an animal travels for its normal daily activities. 
""Minimum Bounding Geometry"" creates a feature class containing polygons which represent a specified 
minimum bounding geometry enclosing each input feature or each group of input features. ""Convex 
hull"" is the smallest convex polygon that can enclose a group of objects, such as a group of points. 
""Kernel Density Mapping"" calculates and visualizes features's density in a given area. ""DBSCAN"", 
Density-Based Spatial Clustering of Applications with Noise that cluster the points based on density 
criterion. 
[Dataset Description]: 
dataset/Elk_in_Southwestern_Alberta_2009.geojson: geojson files for storing points of Elk 
movements in Southwestern Alberta 2009. 

Columns of dataset/Elk_in_Southwestern_Alberta_2009.geojson: 
'OBJECTID', 'timestamp', 'long', 'lat', 'comments', 'external_t', 'dop', 
'fix_type_r', 'satellite_', 'height', 'crc_status', 'outlier_ma', 
'sensor_typ', 'individual', 'tag_ident', 'ind_ident', 'study_name', 
'date', 'time', 'timestamp_Converted', 'summer_indicator', 'geometry' 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
44,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find gap for Toronto fire station service coverage. 
[Instruction]: 
Your task is analyzing Toronto fire stations and their service coverage. The goal is visualize the 
current fire station coverage in Etobicoke by buffering to identify coverage gaps. Visualize the 
most compromised areas in terms of fire service and save the no-service area. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
44,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find gap for Toronto fire station service coverage. 
[Instruction]: 
Your task is analyzing Toronto fire stations and their service coverage. The goal is visualize the 
current fire station coverage in Etobicoke by buffering to identify coverage gaps. Visualize the 
most compromised areas in terms of fire service and save the no-service area. 
[Domain Knowledge]: 
""Service Area"" or ""Service Coverage"" is a region that encompasses all accessible locations within 
the study area. It is usually used to identify how much land, how many people, or how much of anything 
else is within the neighborhood or a region and served by certain facilities. The Overlay toolset 
contains tools to overlay multiple feature classes to combine, erase, modify, or update spatial 
features, resulting in a new feature class. The ""overlay"" function manipulations two feature layers 
using the language of sets, intersection, union, difference and symmetrical difference. ""Buffer"" 
creates buffer polygons or zones around input geometry features to a specified distance in GIS. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
44,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find gap for Toronto fire station service coverage. 
[Instruction]: 
Your task is analyzing Toronto fire stations and their service coverage. The goal is visualize the 
current fire station coverage in Etobicoke by buffering to identify coverage gaps. Visualize the 
most compromised areas in terms of fire service and save the no-service area. 
[Dataset Description]: 
dataset/etobicoke.geojson: Polygon Geojson file for neighborhood region for Etobicoke. 

Columns of dataset/etobicoke.geojson 
'OBJECTID_1', 'LCODE_NAME', 'SECTION', 'DISTRICT', 'DIVISION', 
'OBJECTID', 'Shape_Length', 'Shape_Area', 'geometry' 

dataset/fire_stations.geojson: Point Geojson file for current fire stations distribution as 
points. 

Columns of dataset/fire_stations.geojson: 
'OBJECTID_1', 'NAME', 'ADDRESS', 'WARD_NAME', 'MUN_NAME', 'geometry 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
44,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find gap for Toronto fire station service coverage. 
[Instruction]: 
Your task is analyzing Toronto fire stations and their service coverage. The goal is visualize the 
current fire station coverage in Etobicoke by buffering to identify coverage gaps. Visualize the 
most compromised areas in terms of fire service and save the no-service area. 
[Domain Knowledge]: 
""Service Area"" or ""Service Coverage"" is a region that encompasses all accessible locations within 
the study area. It is usually used to identify how much land, how many people, or how much of anything 
else is within the neighborhood or a region and served by certain facilities. The Overlay toolset 
contains tools to overlay multiple feature classes to combine, erase, modify, or update spatial 
features, resulting in a new feature class. The ""overlay"" function manipulations two feature layers 
using the language of sets, intersection, union, difference and symmetrical difference. ""Buffer"" 
creates buffer polygons or zones around input geometry features to a specified distance in GIS. 

[Dataset Description]: 
dataset/etobicoke.geojson: Polygon Geojson file for neighborhood region for Etobicoke. 

Columns of dataset/etobicoke.geojson 
'OBJECTID_1', 'LCODE_NAME', 'SECTION', 'DISTRICT', 'DIVISION', 
'OBJECTID', 'Shape_Length', 'Shape_Area', 'geometry' 

dataset/fire_stations.geojson: Point Geojson file for current fire stations distribution as 
points. 

Columns of dataset/fire_stations.geojson: 
'OBJECTID_1', 'NAME', 'ADDRESS', 'WARD_NAME', 'MUN_NAME', 'geometry 


[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
45,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is to evaluate mountain lion habitat suitability by calculating terrain ruggedness from 
elevation data. The analysis applies focal statistics with a 3x3 moving window to measure terrain 
variation, followed by rescaling to standardize the ruggedness values. The results are saved in 
the geodatabase. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
45,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is to evaluate mountain lion habitat suitability by calculating terrain ruggedness from 
elevation data. The analysis applies focal statistics with a 3x3 moving window to measure terrain 
variation, followed by rescaling to standardize the ruggedness values. The results are saved in 
the geodatabase. 
[Domain Knowledge]: 
Terrain Ruggedness Index (TRI) expresses the amount of elevation difference between adjacent 
cells of a DEM. The TRI function measures the difference in elevation values from a center cell and 
eight cells directly surrounding it. Then, the eight elevation differences are squared and averaged. 
The square root of this average results is a TRI measurement for the center cell. This calculation 
is then conducted on every cell of the DEM. Distance Accumulation: Calculates accumulated distance 
for each cell to sources, allowing for straight-line distance, cost distance, and true surface 
distance, as well as vertical and horizontal cost factors. Focal Statistic: Calculates for each 
input cell location a statistic of the values within a specified neighborhood around it. Rescale 
by Function: Rescales the input raster values by applying a selected transformation function and 
transforming the resulting values onto a specified continuous evaluation scale. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
45,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is to evaluate mountain lion habitat suitability by calculating terrain ruggedness from 
elevation data. The analysis applies focal statistics with a 3x3 moving window to measure terrain 
variation, followed by rescaling to standardize the ruggedness values. The results are saved in 
the geodatabase. 
[Dataset Description]: 
dataset/Elevation.tif: Raster files for storing elevation data. 
Shape of dataset/Elevation.tif: (2494, 3062) 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
45,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Find optimal corridors to connect dwindling mountain lion populations. 
[Instruction]: 
Your task is to evaluate mountain lion habitat suitability by calculating terrain ruggedness from 
elevation data. The analysis applies focal statistics with a 3x3 moving window to measure terrain 
variation, followed by rescaling to standardize the ruggedness values. The results are saved in 
the geodatabase. 
[Domain Knowledge]: 
Terrain Ruggedness Index (TRI) expresses the amount of elevation difference between adjacent 
cells of a DEM. The TRI function measures the difference in elevation values from a center cell and 
eight cells directly surrounding it. Then, the eight elevation differences are squared and averaged. 
The square root of this average results is a TRI measurement for the center cell. This calculation 
is then conducted on every cell of the DEM. Distance Accumulation: Calculates accumulated distance 
for each cell to sources, allowing for straight-line distance, cost distance, and true surface 
distance, as well as vertical and horizontal cost factors. Focal Statistic: Calculates for each 
input cell location a statistic of the values within a specified neighborhood around it. Rescale 
by Function: Rescales the input raster values by applying a selected transformation function and 
transforming the resulting values onto a specified continuous evaluation scale. 
[Dataset Description]: 
dataset/Elevation.tif: Raster files for storing elevation data. 
Shape of dataset/Elevation.tif: (2494, 3062) 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
46,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Identify hot spots for peak crashes 
[Instruction]: 
Your task is identifying hot spots for peak crashes in Brevard County, Florida, 2010 - 2015. The first 
step is select all the crashes based on peak time zone. Create a copy of selected crashes data. Then, 
snap the crashes points to the road network and spatial join with the road. Calculate the crash rate 
based on the joint data and use hot spot analysis to get crash hot spot map as the result. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
46,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Identify hot spots for peak crashes 
[Instruction]: 
Your task is identifying hot spots for peak crashes in Brevard County, Florida, 2010 - 2015. The first 
step is select all the crashes based on peak time zone. Create a copy of selected crashes data. Then, 
snap the crashes points to the road network and spatial join with the road. Calculate the crash rate 
based on the joint data and use hot spot analysis to get crash hot spot map as the result. 
[Domain Knowledge]: 
We consider traffic between time zone 3pm to 5pm in weekdays as peak. For snap process, the recommend 
buffer on roads is 0.25 miles. Hot spot analysis looks for high crash rates that cluster close together, 
accurate distance measurements based on the road network are essential. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
46,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Identify hot spots for peak crashes 
[Instruction]: 
Your task is identifying hot spots for peak crashes in Brevard County, Florida, 2010 - 2015. The first 
step is select all the crashes based on peak time zone. Create a copy of selected crashes data. Then, 
snap the crashes points to the road network and spatial join with the road. Calculate the crash rate 
based on the joint data and use hot spot analysis to get crash hot spot map as the result. 
[Dataset Description]: 
dataset/crashes.shp: The locations of crashes in Brevard County, Florida between 2010 and 2015. 

dataset/roads.shp: The road network of Brevard County. 
dataset/nwswm360ft.swm: Spatial weights matrix file created using the Generate Network Spatial 
Weights tool and a street network built from Brevard County road polylines. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
46,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Identify hot spots for peak crashes 
[Instruction]: 
Your task is identifying hot spots for peak crashes in Brevard County, Florida, 2010 - 2015. The first 
step is select all the crashes based on peak time zone. Create a copy of selected crashes data. Then, 
snap the crashes points to the road network and spatial join with the road. Calculate the crash rate 
based on the joint data and use hot spot analysis to get crash hot spot map as the result. 
[Domain Knowledge]: 
We consider traffic between time zone 3pm to 5pm in weekdays as peak. For snap process, the recommend 
buffer on roads is 0.25 miles. Hot spot analysis looks for high crash rates that cluster close together, 
accurate distance measurements based on the road network are essential. 
[Dataset Description]: 
dataset/crashes.shp: The locations of crashes in Brevard County, Florida between 2010 and 2015. 

dataset/roads.shp: The road network of Brevard County. 
dataset/nwswm360ft.swm: Spatial weights matrix file created using the Generate Network Spatial 
Weights tool and a street network built from Brevard County road polylines. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
47,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate impervious surface area 
[Instruction]: 
Your task is calculating impervious surface area through the impervious area and lands parcel. 
Use tabulate area to get the area count, and join the area with land parcels, copy and save the joint 
result. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
47,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate impervious surface area 
[Instruction]: 
Your task is calculating impervious surface area through the impervious area and lands parcel. 
Use tabulate area to get the area count, and join the area with land parcels, copy and save the joint 
result. 
[Domain Knowledge]: 
Tabulate area calculate the area through the impervious raster and the land parcels. Then use add 
join function to combine the tabulate area result with land parcels. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
47,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate impervious surface area 
[Instruction]: 
Your task is calculating impervious surface area through the impervious area and lands parcel. 
Use tabulate area to get the area count, and join the area with land parcels, copy and save the joint 
result. 
[Dataset Description]: 
dataset/parcels.shp: The parcels of land in the neighborhood. 
dataset/impervious.tif: Raster file contains the impervious rates of the whole study area. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
47,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Calculate impervious surface area 
[Instruction]: 
Your task is calculating impervious surface area through the impervious area and lands parcel. 
Use tabulate area to get the area count, and join the area with land parcels, copy and save the joint 
result. 
[Domain Knowledge]: 
Tabulate area calculate the area through the impervious raster and the land parcels. Then use add 
join function to combine the tabulate area result with land parcels. 
[Dataset Description]: 
dataset/parcels.shp: The parcels of land in the neighborhood. 
dataset/impervious.tif: Raster file contains the impervious rates of the whole study area. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
48,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Determine how location impacts interest rates 
[Instruction]: 
Your task is to analyze how location impacts interest rates. First, select and copy the tracts with 
at least 30 loans. Find interest rate hot spots on the selected loan tracts. Then create a Generalized 
Weighted Regression model on the selected data to determine how location impacts interest rates. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
48,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Determine how location impacts interest rates 
[Instruction]: 
Your task is to analyze how location impacts interest rates. First, select and copy the tracts with 
at least 30 loans. Find interest rate hot spots on the selected loan tracts. Then create a Generalized 
Weighted Regression model on the selected data to determine how location impacts interest rates. 

[Domain Knowledge]: 
Select the attribute ""AcceptedLo"" larger or equal to 30 in load tracts. The key field in load data 
is ""AveInteres"". In GWR model, use NUMBER_OF_NEIGHBORS as neighborhood_type, and set neighbor 
numbers to 22. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
48,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Determine how location impacts interest rates 
[Instruction]: 
Your task is to analyze how location impacts interest rates. First, select and copy the tracts with 
at least 30 loans. Find interest rate hot spots on the selected loan tracts. Then create a Generalized 
Weighted Regression model on the selected data to determine how location impacts interest rates. 

[Dataset Description]: 
dataset/loan_data.shp: The loan data from August 2007 to September 2015, obtained from LendingClub 
Statistics. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
48,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Determine how location impacts interest rates 
[Instruction]: 
Your task is to analyze how location impacts interest rates. First, select and copy the tracts with 
at least 30 loans. Find interest rate hot spots on the selected loan tracts. Then create a Generalized 
Weighted Regression model on the selected data to determine how location impacts interest rates. 

[Domain Knowledge]: 
Select the attribute ""AcceptedLo"" larger or equal to 30 in load tracts. The key field in load data 
is ""AveInteres"". In GWR model, use NUMBER_OF_NEIGHBORS as neighborhood_type, and set neighbor 
numbers to 22. 
[Dataset Description]: 
dataset/loan_data.shp: The loan data from August 2007 to September 2015, obtained from LendingClub 
Statistics. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
49,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Mapping the Impact of Housing Shortage on Oil Workers 
[Instruction]: 
Your task is to analyze homelessness trends in North Dakota during the 2013 oil boom. Join homelessness 
tabular data to the U.S. states spatial layer using state abbreviations. Then, save the maps for 
analysis. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
49,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Mapping the Impact of Housing Shortage on Oil Workers 
[Instruction]: 
Your task is to analyze homelessness trends in North Dakota during the 2013 oil boom. Join homelessness 
tabular data to the U.S. states spatial layer using state abbreviations. Then, save the maps for 
analysis. 
[Domain Knowledge]: 
Tabular Data Joining: Joining tabular data to spatial layers using a common field (e.g., state abbreviation) 
is crucial for mapping. The North Dakota oil boom (2012–2013) attracted thousands of workers due 
to high-paying jobs in the energy sector. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
49,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Mapping the Impact of Housing Shortage on Oil Workers 
[Instruction]: 
Your task is to analyze homelessness trends in North Dakota during the 2013 oil boom. Join homelessness 
tabular data to the U.S. states spatial layer using state abbreviations. Then, save the maps for 
analysis. 
[Dataset Description]: 
dataset/homeless_data.xlsx: containing state-level homelessness data in the United States 
for 2013. The dataset includes the following columns:State, Change, Pop13, Homeless13 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
49,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Mapping the Impact of Housing Shortage on Oil Workers 
[Instruction]: 
Your task is to analyze homelessness trends in North Dakota during the 2013 oil boom. Join homelessness 
tabular data to the U.S. states spatial layer using state abbreviations. Then, save the maps for 
analysis. 
[Domain Knowledge]: 
Tabular Data Joining: Joining tabular data to spatial layers using a common field (e.g., state abbreviation) 
is crucial for mapping. The North Dakota oil boom (2012–2013) attracted thousands of workers due 
to high-paying jobs in the energy sector. 
[Dataset Description]: 
dataset/homeless_data.xlsx: containing state-level homelessness data in the United States 
for 2013. The dataset includes the following columns:State, Change, Pop13, Homeless13 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
50,workflow,False,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Predict seagrass habitats 
[Instruction]: 
Your task is to predict seagrass habitats with machine learning methods. First, dissolve the USA 
seagrass, and create 5000 random points on the dissolved result. Then, use the empirical Bayesian 
kriging geostatistical method to interpolate raster surfaces for the environmental values stored 
in the Global ocean measurements layer, for temp and salinity, separately. Finally, use the two 
kriging result as explanatory rasters to perform presence-only prediction (MaxEnt), get the train 
features and seagrass predict raster as the outputs. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
50,workflow,True,False,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Predict seagrass habitats 
[Instruction]: 
Your task is to predict seagrass habitats with machine learning methods. First, dissolve the USA 
seagrass, and create 5000 random points on the dissolved result. Then, use the empirical Bayesian 
kriging geostatistical method to interpolate raster surfaces for the environmental values stored 
in the Global ocean measurements layer, for temp and salinity, separately. Finally, use the two 
kriging result as explanatory rasters to perform presence-only prediction (MaxEnt), get the train 
features and seagrass predict raster as the outputs. 
[Domain Knowledge]: 
Empirical Bayesian kriging (EBK) is a geostatistical interpolation method that automates the 
most difficult aspects of building a valid kriging model. The Presence-only Prediction (MaxEnt) 
tool uses a maximum entropy approach (MaxEnt) to estimate the probability of presence of a phenomenon. 
The tool uses known occurrence points and explanatory variables in the form of fields, rasters, 
or distance features to provide an estimate of presence across a study area. 
[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
50,workflow,False,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Predict seagrass habitats 
[Instruction]: 
Your task is to predict seagrass habitats with machine learning methods. First, dissolve the USA 
seagrass, and create 5000 random points on the dissolved result. Then, use the empirical Bayesian 
kriging geostatistical method to interpolate raster surfaces for the environmental values stored 
in the Global ocean measurements layer, for temp and salinity, separately. Finally, use the two 
kriging result as explanatory rasters to perform presence-only prediction (MaxEnt), get the train 
features and seagrass predict raster as the outputs. 
[Dataset Description]: 
dataset/Global_ocean_measurements.shp: Ecological Marine Units point data that contains ocean 
measurements up to a 90-meter water depth. 
dataset/Global_shallow_waters.shp: Global shallow bathymetry polygon used to predict seagrass 
globally. 
dataset/USA_seagrass.shp: For seagrass occurrence, every polygon in USA seagrass is an identified 
seagrass habitat. 
dataset/USA_shallow_waters.shp: Shallow bathymetry polygon for the continental United States 
used as the study area for model training. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
50,workflow,True,True,True,"As a Geospatial data scientist, you will generate a workflow to a proposed task.

[Task]: 
Predict seagrass habitats 
[Instruction]: 
Your task is to predict seagrass habitats with machine learning methods. First, dissolve the USA 
seagrass, and create 5000 random points on the dissolved result. Then, use the empirical Bayesian 
kriging geostatistical method to interpolate raster surfaces for the environmental values stored 
in the Global ocean measurements layer, for temp and salinity, separately. Finally, use the two 
kriging result as explanatory rasters to perform presence-only prediction (MaxEnt), get the train 
features and seagrass predict raster as the outputs. 
[Domain Knowledge]: 
Empirical Bayesian kriging (EBK) is a geostatistical interpolation method that automates the 
most difficult aspects of building a valid kriging model. The Presence-only Prediction (MaxEnt) 
tool uses a maximum entropy approach (MaxEnt) to estimate the probability of presence of a phenomenon. 
The tool uses known occurrence points and explanatory variables in the form of fields, rasters, 
or distance features to provide an estimate of presence across a study area. 
[Dataset Description]: 
dataset/Global_ocean_measurements.shp: Ecological Marine Units point data that contains ocean 
measurements up to a 90-meter water depth. 
dataset/Global_shallow_waters.shp: Global shallow bathymetry polygon used to predict seagrass 
globally. 
dataset/USA_seagrass.shp: For seagrass occurrence, every polygon in USA seagrass is an identified 
seagrass habitat. 
dataset/USA_shallow_waters.shp: Shallow bathymetry polygon for the continental United States 
used as the study area for model training. 

[Key Notes]:
1.Use **automatic reasoning** and clearly explain each step (Chain of Thoughts approach).
2.Using **NetworkX* package for visualization.
3.Using 'dot' for graph visualization layout.
4.Multiple subtasks can be proceeded correspondingly because
all of their outputs will be inputs for the next subtask.
5.Limiting your output to code, no extra information.
6.Only codes for workflow, no implementation.

[Expected Sample Output Begin]
 
""""""
  tasks = [“task1”, “task2”, “task3”]

  G = nx.DiGraph()
  for i in range(len(tasks) - 1):
      G.add_edge(tasks[i], tasks[i + 1])
  pos = nx.drawing.nx_pydot.graphviz_layout(G, prog=""dot"")
  plt.figure(figsize=(15, 8))
  nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
  plt.title(""Workflow for Analyzing Urban Heat Using Kriging Interpolation"", fontsize=14)
  plt.show()
""""""
  [Expected Sample Output End]"
